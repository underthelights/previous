---
layout: article
title: Introduction To PyTorch
tags: PyTorch AI
category: PyTorch AI
picture_frame: shadow

---

**[Introduction to PyTorch](https://docs.microsoft.com/en-us/learn/modules/intro-machine-learning-pytorch/)**  
by [PyTorch Fundamentals @ Mircosoft Learn](https://docs.microsoft.com/en-us/learn/paths/pytorch-fundamentals/)
<!--more-->

### TOC

- [PyTorch Fundamentals](https://docs.microsoft.com/learn/paths/pytorch-fundamentals/)
- [Introduction](https://docs.microsoft.com/en-us/learn/modules/intro-machine-learning-pytorch/1-introduction)1 min
- [What are Tensors?](https://docs.microsoft.com/en-us/learn/modules/intro-machine-learning-pytorch/2-tensors)10 min
- [Load data with PyTorch Datasets and DataLoaders](https://docs.microsoft.com/en-us/learn/modules/intro-machine-learning-pytorch/3-data)10 min
- [Transform the data](https://docs.microsoft.com/en-us/learn/modules/intro-machine-learning-pytorch/4-transforms)5 min
- [Building the model layers](https://docs.microsoft.com/en-us/learn/modules/intro-machine-learning-pytorch/5-model)15 min
- [Automatic differentiation](https://docs.microsoft.com/en-us/learn/modules/intro-machine-learning-pytorch/6-autograd)10 min
- [Learn about the optimization loop](https://docs.microsoft.com/en-us/learn/modules/intro-machine-learning-pytorch/7-optimization)15 min
- [Save, load, and run model predictions](https://docs.microsoft.com/en-us/learn/modules/intro-machine-learning-pytorch/8-inference)8 min
- [The full model building process](https://docs.microsoft.com/en-us/learn/modules/intro-machine-learning-pytorch/9-quickstart)15 min
- [Summary](https://docs.microsoft.com/en-us/learn/modules/intro-machine-learning-pytorch/10-summary)

# Introduction

Completed100 XP

- 1 minute

Most machine learning workflows involve working with data, creating models, optimizing model parameters, and saving the trained models. This module introduces you to a complete machine learning (ML) workflow implemented in [PyTorch](https://pytorch.org/), a popular ML framework for Python.

대부분의 기계 학습 워크플로우에는 데이터 작업, 모델 생성, 모델 파라미터 최적화, 교육 받은 모델 저장 등이 포함됩니다. 이 모듈에서는 인기 있는 Python용 Framework인 PyTorch를 활용하여 완벽한 ML(머신러닝) 워크플로우를 소개합니다. 

We’ll use the FashionMNIST dataset to train a neural network that predicts if an input image belongs to one of the following classes: T-shirt/top, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, or Ankle boot.

Before we jump into building the model, you will learn a set of core concepts required to understand the basics of building Machine Learning models. Then the last step will put it all together.

FashionMNIST 데이터셋을 사용하여 입력 이미지가 다음 클래스 중 하나에 속하는지 여부를 예측하는 신경 네트워크를 교육할 것입니다. 모델을 구축하기 전에 먼저 기계 학습 모델 구축의 기본을 이해하는 데 필요한 핵심 개념을 배우실 것입니다. 그러면 마지막 단계로 모든 것이 정리될 것입니다.

## Learning objectives

In this module you will:

- Learn the key concepts used to build machine learning models M
- Learn how to build a Computer Vision model
- Build models with the PyTorch API

## Prerequisites

- Basic Python knowledge
- Basic knowledge about how to use Jupyter Notebooks

----

# What are Tensors?

Completed200 XP

- 10 minutes

```python
%matplotlib inline
```

# Tensors

Tensors are a specialized data structure that are very similar to arrays and matrices. In PyTorch, we use tensors to encode the inputs and outputs of a model, as well as the model’s parameters.

Tensor는 배열과 행렬에 매우 유사한 특수 자료구조이다. PyTorch에서는 Tensor를 활용하여 모델의 입출력과 모델의 parameter을 인코딩한다.

Tensors are similar to [NumPy’s](https://numpy.org/) ndarrays, except that tensors can run on GPUs or other hardware accelerators. In fact, tensors and NumPy arrays can often share the same underlying memory, eliminating the need to copy data (see `bridge-to-np-label`). Tensors are also optimized for automatic differentiation (we'll see more about that later in the Autograd unit). If you’re familiar with `ndarrays`, you’ll be right at home with the Tensor API. If not, follow along!

Tensor는 NumPy랑 유사하지만 GPU 또는 기타 하드웨어 가속기에서 실행할 수 있습니다. 실제로 텐서와 NumPy 어레이는 종종 동일한 기본 메모리를 공유할 수 있으므로 데이터를 복사할 필요가 없습니다('bridge-to-np-label' 참조). 텐서는 또한 자동 분화에 최적화되어 있습니다(오토그라드 장치의 뒷부분에서 자세히 살펴보겠습니다). "ndarrays"에 익숙하다면 Tensor API를 바로 사용할 수 있을 것입니다. 그렇지 않다면, 따라오세요!

먼저 환경을 설정하는 것부터 시작하겠습니다.

Let's start by setting up our environment.

```python
import torch

import numpy as np
```

# Initializing a Tensor

Tensors can be initialized in various ways. Take a look at the following examples:



## Directly from data

Tensors can be created directly from data. The data type is automatically inferred.

```python
data = [[1, 2],[3, 4]]
x_data = torch.tensor(data)
```



## From a NumPy array

Tensors can be created from NumPy arrays (and vice versa - see `bridge-to-np-label`).

```python
np_array = np.array(data)
x_np = torch.from_numpy(np_array)
```



## From another tensor:

The new tensor retains the properties (shape, data type) of the argument tensor, unless explicitly overridden.

```python
x_ones = torch.ones_like(x_data) # retains the properties of x_data
print(f"Ones Tensor: \n {x_ones} \n")

x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data
print(f"Random Tensor: \n {x_rand} \n")
```



## With random or constant values:

`shape` is a tuple of tensor dimensions. In the functions below, it determines the dimensionality of the output tensor.

```python
shape = (2,3,)
rand_tensor = torch.rand(shape)
ones_tensor = torch.ones(shape)
zeros_tensor = torch.zeros(shape)

print(f"Random Tensor: \n {rand_tensor} \n")
print(f"Ones Tensor: \n {ones_tensor} \n")
print(f"Zeros Tensor: \n {zeros_tensor}")		
```

# Attributes of a Tensor

Tensor attributes describe their shape, data type, and the device on which they are stored.

```python
tensor = torch.rand(3,4)

print(f"Shape of tensor: {tensor.shape}")
print(f"Datatype of tensor: {tensor.dtype}")
print(f"Device tensor is stored on: {tensor.device}")
```

# Operations on Tensors

Over 100 tensor operations, including arithmetic, linear algebra, matrix manipulation (transposing, indexing, slicing), sampling and more are comprehensively described [here](https://pytorch.org/docs/stable/torch.html). Each of these operations can be run on the GPU (at typically higher speeds than on a CPU). By default, tensors are created on the CPU. We need to explicitly move tensors to the GPU using `.to` method (after checking for GPU availability). Keep in mind that copying large tensors across devices can be expensive in terms of time and memory!

산술, 선형 대수, 매트릭스 조작(합성, 인덱싱, 슬라이싱)을 포함한 100개 이상의 텐서 연산이 [여기](https://pytorch.org/docs/stable/torch.html))에 포괄적으로 설명되어 있다. 이러한 각 작업은 CPU보다 일반적으로 더 빠른 속도로 GPU에서 실행할 수 있습니다. 기본적으로 tensors은 CPU에. 우리는 명시적으로 GPU에`.to` Method(GPU공급에 대한 조사한 후)을 사용하여 tensors 옮겨야 해 만들어집니다. 기기 간에 대형 텐서를 복사하는 것은 시간과 메모리 측면에서 비용이 많이 들 수 있다는 점을 유념하십시오.

```python
# We move our tensor to the GPU if available
if torch.cuda.is_available():
 tensor = tensor.to('cuda')
```

Try out some of the operations from the list. If you're familiar with the NumPy API, you'll find the Tensor API a breeze to use.

## Standard numpy-like indexing and slicing:

```python
tensor = torch.ones(4, 4)

print('First row: ',tensor[0])
print('First column: ', tensor[:, 0])
print('Last column:', tensor[..., -1])
tensor[:,1] = 0
print(tensor)
```

## Joining tensors

You can use `torch.cat` to concatenate a sequence of tensors along a given dimension. See also [torch.stack](https://pytorch.org/docs/stable/generated/torch.stack.html), another tensor joining op that is subtly different from `torch.cat`.

torch.cat을 사용하여 특정 차원의 텐서 시퀀스를 연결할 수 있습니다. 또한 [filename.stack](https://pytorch.org/docs/stable/generated/torch.stack.html), op에 참여하는 또 다른 텐서 'torch.cat'과는 미묘하게 다른 텐서를 참조하십시오.

```python
t1 = torch.cat([tensor, tensor, tensor], dim=1)
print(t1)
```

## Arithmetic operations

```python
# This computes the matrix multiplication between two tensors. y1, y2, y3 will have the same value
y1 = tensor @ tensor.T
y2 = tensor.matmul(tensor.T)
y3 = torch.rand_like(tensor)

torch.matmul(tensor, tensor.T, out=y3)

# This computes the element-wise product. z1, z2, z3 will have the same value
z1 = tensor * tensor
z2 = tensor.mul(tensor)
z3 = torch.rand_like(tensor)

torch.mul(tensor, tensor, out=z3)
```



## Single-element tensors

If you have a one-element tensor, for example by aggregating all values of a tensor into one value, you can convert it to a Python numerical value using `item()`:

예를 들어 텐서의 모든 값을 하나의 값으로 집계하는 등 단일 요소 텐서가 있는 경우 '항목()'을 사용하여 파이썬 숫자 값으로 변환할 수 있습니다.

```python
agg = tensor.sum()
agg_item = agg.item() 
print(agg_item, type(agg_item))
```



## In-place operations

Operations that store the result into the operand are called in-place. They are denoted by a `_` suffix. For example: `x.copy_(y)`, `x.t_()`, will change `x`.

결과를 피연산자에 저장하는 작업을 인플레이스라고 합니다. 접미사는 '_'입니다. 예를 들어, 'x.copy_(y)', 'x.t_()'는 'x'를 변경합니다.

> **Note:** In-place operations save some memory, but can be problematic when computing derivatives because of an immediate loss of history. Hence, their use is discouraged.
>
> **참고:** 인플레이스 작업은 메모리를 절약하지만 파생 모델 계산 시 즉각적인 이력 손실로 인해 문제가 발생할 수 있습니다. 따라서 사용을 금지합니다.

```python
print(tensor, "\n")
tensor.add_(5)
print(tensor)
```



## Bridge with NumPy

Tensors on the CPU and NumPy arrays can share their underlying memory locations, and changing one will change the other. CPU 및 NumPy 어레이의 텐서는 기본 메모리 위치를 공유할 수 있으며 하나를 변경하면 다른 하나가 변경됩니다.

### Tensor to NumPy array

```python
t = torch.ones(5)
print(f"t: {t}")

n = t.numpy()
print(f"n: {n}")
```

A change in the tensor reflects in the NumPy array.

```python
t.add_(1)

print(f"t: {t}")
print(f"n: {n}")
```

### NumPy array to Tensor

```python
n = np.ones(5)
t = torch.from_numpy(n)
```

Changes in the NumPy array reflects in the tensor.

```python
np.add(n, 1, out=n)

print(f"t: {t}")
print(f"n: {n}")
```



## Check your knowledge

1. Which is true of Tensors? 다음 중 텐서에게 맞는 것은?
   - Tensors are a string type representing a vector. 텐서는 벡터를 나타내는 문자열 유형입니다.
   - Tensors are a mathematical value in Python to represent GPS coordinates. 텐서는 GPS 좌표를 나타내는 파이썬의 수학적 값입니다.
   - Tensors are specialized data structures that are similar to arrays and matrices. 텐서는 배열 및 매트릭스와 유사한 특수 데이터 구조입니다.

- Check your answers

----

# Load data with PyTorch Datasets and DataLoaders

```python
%matplotlib inline
```

# Datasets and Dataloaders

Code for processing data samples can get messy and hard to maintain; we ideally want our dataset code to be decoupled from our model training code for better readability and modularity. PyTorch provides two data primitives: `torch.utils.data.DataLoader` and `torch.utils.data.Dataset` that allow you to use pre-loaded datasets as well as your own data. `Dataset` stores the samples and their corresponding labels, and `DataLoader` wraps an iterable around the `Dataset` to enable easy access to the samples.

데이터 샘플을 처리하는 코드가 복잡하고 유지 관리가 어려울 수 있습니다. 데이터셋 코드를 모델 교육 코드에서 분리하여 가독성과 모듈성을 향상하는 것이 이상적입니다. PyTorch는 두 가지 데이터 기본 요소인 `torch.utils.data` 를 제공합니다. `DataLoader` 및 `torch.utils.data.Dataset` 사전 로드된 데이터셋과 사용자 고유의 데이터를 사용할 수 있는 데이터셋입니다. 데이터셋은 샘플과 해당 라벨을 저장하고, 데이터로더는 데이터셋에 쉽게 접근할 수 있도록 데이터셋 주위에 감싼다.



PyTorch domain libraries provide a number of pre-loaded datasets (such as FashionMNIST) that subclass `torch.utils.data.Dataset` and implement functions specific to the particular data. They can be used to prototype and benchmark your model. You can find them here: [Image Datasets](https://pytorch.org/vision/stable/datasets.html), [Text Datasets](https://pytorch.org/text/stable/datasets.html), and [Audio Datasets](https://pytorch.org/audio/stable/datasets.html)

PyTorch 도메인 라이브러리는 'torch.utils.data' 하위 클래스를 포함하는 많은 사전 로드된 데이터 세트(예: FashionMNIST)를 제공합니다.데이터 세트' 및 특정 데이터 전용 기능 구현. 모델을 시제품으로 제작하고 벤치마킹하는 데 사용할 수 있습니다. 다음에서 찾을 수 있습니다. [이미지 데이터 세트](https://pytorch.org/vision/stable/datasets.html), [텍스트 데이터 세트])(https://pytorch.org/text/stable/datasets.html), 및 [오디오 데이터 세트](https://pytorch.org/audio/stable/datasets.html))



## Loading a dataset

Here is an example of how to load the [Fashion-MNIST](https://research.zalando.com/welcome/mission/research-projects/fashion-mnist/) dataset from TorchVision. Fashion-MNIST is a dataset of Zalando’s article images consisting of of 60,000 training examples and 10,000 test examples. Each example comprises a 28×28 grayscale image and an associated label from one of 10 classes.

다음은 TorchVision에서 [Fashion-MNIST](https://research.zalando.com/welcome/mission/research-projects/fashion-mnist/) 데이터 세트를 로드하는 방법의 예입니다. Fashion-MNIST는 60,000개의 교육 사례와 10,000개의 테스트 사례로 구성된 잘란도의 기사 이미지 데이터 세트입니다. 각 예는 28×28 그레이스케일 영상과 10개 클래스 중 하나의 관련 라벨로 구성됩니다.

We load the [FashionMNIST Dataset](https://pytorch.org/vision/stable/datasets.html#fashion-mnist) with the following parameters: 

다음 매개 변수를 사용하여 [FashionMNIST Dataset](https://pytorch.org/vision/stable/datasets.html#fashion-mnist)을 로드합니다.

- `root` is the path where the train/test data is stored, 열차/테스트 데이터가 저장되는 경로
- `train` specifies training or test dataset, 교육 또는 테스트 데이터 세트
- `download=True` downloads the data from the Internet if it's not available at `root`. 인터넷에서 데이터를 다운로드
- `transform` and `target_transform` specify the feature and label transformations 

```python
import torch
from torch.utils.data import Dataset
from torchvision import datasets
from torchvision.transforms import ToTensor, Lambda
import matplotlib.pyplot as plt

training_data = datasets.FashionMNIST(
  root="data",
  train=True,
  download=True,
  transform=ToTensor()
)

test_data = datasets.FashionMNIST(
  root="data",
  train=False,
  download=True,
  transform=ToTensor()
)
```

- 결과

```
Using downloaded and verified file: 
data/FashionMNIST/raw/train-images-idx3-ubyte.gz Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw Using downloaded and verified file: data/FashionMNIST/raw/train-labels-idx1-ubyte.gz Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw 

Using downloaded and verified file: 
data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw Using downloaded and verified file: data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw Processing... Done! ``/anaconda/envs/py37_pytorch/lib/python3.7/site-packages/torchvision/datasets/mnist.py:480: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1607370141920/work/torch/csrc/utils/tensor_numpy.cpp:141.)  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s) 
```

## Iterating and Visualizing the Dataset

We can index `Datasets` manually like a list: `training_data[index]`. We use `matplotlib` to visualize some samples in our training data.

목록처럼 '데이터셋'을 수동으로 인덱싱할 수 있습니다. 'training_data[index]\" 우리는 훈련 데이터에서 일부 샘플을 시각화하기 위해 '매트플롯립'을 사용한다.

```python
labels_map = {
  0: "T-Shirt",
  1: "Trouser",
  2: "Pullover",
  3: "Dress",
  4: "Coat",
  5: "Sandal",
  6: "Shirt",
  7: "Sneaker",
  8: "Bag",
  9: "Ankle Boot",
}
figure = plt.figure(figsize=(8, 8))
cols, rows = 3, 3

for i in range(1, cols * rows + 1):
  sample_idx = torch.randint(len(training_data), size=(1,)).item()
  img, label = training_data[sample_idx]
  figure.add_subplot(rows, cols, i)
  plt.title(labels_map[label])
  plt.axis("off")
  plt.imshow(img.squeeze(), cmap="gray")
  
plt.show()
```



![img](data:image/jpeg;base64,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)

## Creating a Custom Dataset for your files

A custom Dataset class must implement three functions: `__init__`, `__len__`, and `__getitem__`. Take a look at this implementation; the FashionMNIST images are stored in a directory `img_dir`, and their labels are stored separately in a CSV file `annotations_file`.

사용자 지정 Dataset 클래스는 세 가지 함수를 구현해야 합니다. : `__init__`, `__len__`, and `__getitem__` FashionMNIST 이미지는 디렉토리 'img_dir'에 저장되고 레이블은 CSV 파일 'annotations_file'에 별도로 저장됩니다.

In the next sections, we'll break down what's happening in each of these functions.

```python
import os
import pandas as pd
import torchvision.io as tvio
class CustomImageDataset(Dataset):
  def init(self, annotations_file, img_dir, transform=None, target_transform=None):
    self.img_labels = pd.read_csv(annotations_file)
    self.img_dir = img_dir
    self.transform = transform
    self.target_transform = target_transform 
	
  def len(self):
    return len(self.img_labels)

  def getitem(self, idx):
    img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])
    image = tvio.read_image(img_path)
    label = self.img_labels.iloc[idx, 1]
    
    if self.transform:
      image = self.transform(image)
      
		if self.target_transform:
      label = self.target_transform(label)
      sample = {"image": image, "label": label}

		return sample
```



## init

The `__init__` function is run once when instantiating the Dataset object. We initialize the directory containing the images, the annotations file, and both transforms (covered in more detail in the next section).

The labels.csv file looks like:

```
    tshirt1.jpg, 0
    tshirt2.jpg, 0
    ......
    ankleboot999.jpg, 9
```

Example:

```python
def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):
    self.img_labels = pd.read_csv(annotations_file)
    self.img_dir = img_dir
    self.transform = transform
    self.target_transform = target_transform
```



## len

The `__len__` function returns the number of samples in our dataset.

Example:

```python
def __len__(self):
    return len(self.img_labels)
```

## getitem

The `__getitem__` function loads and returns a sample from the dataset at the given index `idx`. Based on the index, it identifies the image's location on disk, converts that to a tensor using `read_image`, retrieves the corresponding label from the csv data in `self.img_labels`, calls the transform functions on them (if applicable), and returns the tensor image and corresponding label in a Python `dict`.

Example:

```python
def __getitem__(self, idx):
    img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])
    image = read_image(img_path)
    label = self.img_labels.iloc[idx, 1]
    if self.transform:
        image = self.transform(image)
    if self.target_transform:
        label = self.target_transform(label)
    sample = {"image": image, "label": label}
    return sample
```



## Preparing your data for training with DataLoaders

The `Dataset` retrieves our dataset's features and labels one sample at a time. While training a model, we typically want to pass samples in "minibatches", reshuffle the data at every epoch to reduce model overfitting, and use Python's `multiprocessing` to speed up data retrieval.

Dataset은 데이터셋의 기능과 레이블을 한 번에 하나씩 검색합니다. 모델을 교육하는 동안 우리는 일반적으로 "미니배치"에서 샘플을 통과시키고, 모델 오버핏을 줄이기 위해 모든 시기에서 데이터를 교체하고, 데이터 검색 속도를 높이기 위해 파이썬의 "다중 처리"를 사용합니다.

`DataLoader` is an iterable that abstracts this complexity for us in an easy API.

```python
from torch.utils.data import DataLoader
train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)
test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)
```

```
--------------------------------------------------------------------------- NameError                                 
Traceback (most recent call last) <ipython-input-1-11332317b96f> in <module>      1 from torch.utils.data import DataLoader     
2  ----> 
3 train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)      4 test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True) NameError: name 'training_data' is not defined
```



## Iterate through the DataLoader

We have loaded that dataset into the `Dataloader` and can iterate through the dataset as needed. Each iteration below returns a batch of `train_features` and `train_labels`(containing `batch_size=64` features and labels respectively). Because we specified `shuffle=True`, after we iterate over all batches the data is shuffled (for finer-grained control over the data loading order, take a look at [Samplers](https://pytorch.org/docs/stable/data.html#data-loading-order-and-sampler>).

이 데이터 세트를 'Dataloader'에 로드했으며 필요에 따라 데이터 세트를 통해 반복할 수 있습니다. 아래의 각 반복은 'train_size'와 'train_size=64' 기능 및 레이블을 각각 포함하는 'train_size' 배치를 반환합니다. =를 지정했기 때문에true'는 모든 배치에 대해 반복한 후 데이터를 섞습니다(데이터 로딩 순서를 세부적으로 제어하려면 [샘플러](https://pytorch.org/docs/stable/data.html#data-loading-order-and-sampler >).

```python
# Display image and label.
train_features, train_labels = next(iter(train_dataloader))
print(f"Feature batch shape: {train_features.size()}")
print(f"Labels batch shape: {train_labels.size()}")

img = train_features[0].squeeze()
label = train_labels[0]
plt.imshow(img, cmap="gray")
plt.show()
print(f"Label: {label}")
```

## Check your knowledge

1. What is the difference between a PyTorch `DataSet` and a PyTorch `DataLoader`
   - A `DataSet` is designed to work with *batches* of data while a `DataLoader` is designed for retrieval of *individual* data items.
   - A `DataSet` is designed for retrieval of *individual* data items while a `DataLoader` is designed to work with *batches* of data.
     - Correct, a `DataSet` is designed for retrieval of *individual* data items while a `DataLoader` is designed to work with *batches* of data.
   - The `DataLoader` class is the parent of the `DataSet` class
   - The `DataSet` class is the parent of the `DataLoader` class

---

# Transform the data

```python
%matplotlib inline
```

# Transforms

Data does not always come in its final processed form that is required for training machine learning algorithms. We use **transforms** to perform some manipulation of the data and make it suitable for training.

데이터는 기계 학습 알고리즘을 교육하는 데 필요한 최종 처리 형식으로만 제공되지는 않습니다. **변환**을(를) 사용하여 일부 데이터 조작을 수행하고 교육에 적합합니다.

All TorchVision datasets have two parameters (`transform` to modify the features and `target_transform` to modify the labels) that accept callables containing the transformation logic. The [torchvision.transforms](https://pytorch.org/vision/stable/transforms.html) module offers several commonly-used transforms out of the box.

모든 TorchVision 데이터 세트에는 변환 논리가 포함된 캘러블을 허용하는 두 개의 매개 변수(특징을 수정하려면 '변환'을, 레이블을 수정하려면 'target_transform'을 사용합니다. [tftionvision.tfts](https://pytorch.org/vision/stable/transforms.html) 모듈은 포장에서 꺼낸 즉시 자주 사용되는 몇 가지 변환 기능을 제공합니다.

The FashionMNIST features are in PIL Image format, and the labels are integers. For training, we need the features as normalized tensors, and the labels as one-hot encoded tensors. To make these transformations, we use `ToTensor` and `Lambda`.

FashionMNIST 기능은 PIL 이미지 형식이며 라벨은 정수입니다. 교육을 위해 표준화된 텐서와 원핫 인코딩 텐서로 된 라벨이 필요합니다. 이러한 변신을 위해 우리는 'To Tensor'와 'Lambda'를 사용한다.

```python
from torchvision import datasets
from torchvision.transforms import ToTensor, Lambda

ds = datasets.FashionMNIST(
  root="data",
  train=True,
  download=True,
  transform=ToTensor(),
  target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))
)
```



## ToTensor()

[ToTensor](https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.ToTensor) converts a PIL image or NumPy `ndarray` into a `FloatTensor` and scales the image's pixel intensity values in the range [0., 1.]

[ToTensor](https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.ToTensor)는 PIL 이미지 또는 NumPy 'ndarray'를 'FloatTensor'로 변환하고 이미지의 픽셀 명암 값을 범위 내에서 조정합니다[0., 1.].



## Lambda transforms

Lambda transforms apply any user-defined lambda function. Here, we define a function to turn the integer into a one-hot encoded tensor. It first creates a zero tensor of size 10 (the number of labels in our dataset) and calls [scatter](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.scatter_) which assigns a `value=1` on the index as given by the label `y`.

람다 변환은 사용자 정의 람다 함수를 적용합니다. 여기서는 정수를 원핫 인코딩 텐서로 변환하는 함수를 정의합니다. 먼저 크기가 10인 제로 텐서(데이터셋의 라벨 수)를 생성하고 [datastore](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.scatter_))를 호출하여 라벨 'y'가 지정한 인덱스에 '값=1'을 할당합니다.

```python
target_transform = Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1))
```



## Check your knowledge

1. Transforms in PyTorch are designed to: 
   - perform some manipulation of the data to make it suitable for training [This is correct!
   - convert input tensors into an output tensor that contains a prediction
   - convert data items into visual representations

---

# Building the model layers

```python
%matplotlib inline
```

# Build a neural network

Neural networks comprise of layers/modules that perform operations on data. The [torch.nn](https://pytorch.org/docs/stable/nn.html) namespace provides all the building blocks you need to build your own neural network. Every module in PyTorch subclasses the [nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). A neural network is a module itself that consists of other modules (layers). This nested structure allows for building and managing complex architectures easily.

신경 네트워크는 데이터에 대한 작업을 수행하는 계층/모듈로 구성됩니다. [filename.nn](https://pytorch.org/docs/stable/nn.html) 네임스페이스는 고유한 신경 네트워크를 구축하는 데 필요한 모든 구성 요소를 제공합니다. PyTorch의 모든 모듈은 [nn]을 하위 분류합니다.모듈](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). 신경망은 다른 모듈(계층)로 구성된 모듈 그 자체입니다. 이러한 중첩된 구조를 통해 복잡한 아키텍처를 쉽게 구축하고 관리할 수 있습니다.

In the following sections, we'll build a neural network to classify images in the FashionMNIST dataset.다음 섹션에서는 FashionMNIST 데이터 세트의 이미지를 분류하는 신경 네트워크를 구축하겠습니다.

```python
import os
import torch

from torch import nn
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
```



## Get a hardware device for training

We want to be able to train our model on a hardware accelerator like the GPU, if it is available. Let's check to see if [torch.cuda](https://pytorch.org/docs/stable/notes/cuda.html) is available, else we continue to use the CPU. GPU와 같은 하드웨어 가속기가 가능하다면 모델을 교육할 수 있기를 바랍니다. 이제 [torch.cuda](https://pytorch.org/docs/stable/notes/cuda.html)를 사용할 수 있는지 확인하고 CPU를 계속 사용하겠습니다.

```python
device = 'cuda' if torch.cuda.is_available() else 'cpu'
print('Using {} device'.format(device))
```

## Define the class

We define our neural network by subclassing `nn.Module`, and initialize the neural network layers in `__init__`. Every `nn.Module` subclass implements the operations on input data in the `forward` method.

우리는 nn을 세분화하여 우리의 신경망을 정의한다.모듈' 및 '_init__'에서 신경 네트워크 계층을 초기화합니다. 매 nn마다.Module' 하위 클래스는 입력 데이터에 대한 작업을 'Forward' 방식으로 구현합니다.

```python
class NeuralNetwork(nn.Module):
  def __init__(self):
    super(NeuralNetwork, self).__init__()
    self.flatten = nn.Flatten()
    self.linear_relu_stack = nn.Sequential(
      nn.Linear(28*28, 512),
      nn.ReLU(),
      nn.Linear(512, 512),
      nn.ReLU(),
      nn.Linear(512, 10),
      nn.ReLU()
    )
	
  def forward(self, x):
    x = self.flatten(x)
    logits = self.linear_relu_stack(x)
    return logits
```



We create an instance of `NeuralNetwork`, and move it to the `device`, and print it's structure.

```python
model = NeuralNetwork().to(device)
print(model)
```

```markdown
NeuralNetwork(  
	(flatten): Flatten(start_dim=1, end_dim=-1) 
  (linear_relu_stack): Sequential(   
    (0): Linear(in_features=784, out_features=512, bias=True)   
    (1): ReLU()    
    (2): Linear(in_features=512, out_features=512, bias=True)  
    (3): ReLU()  
    (4): Linear(in_features=512, out_features=10, bias=True)   
    (5): ReLU()  
  )
) 
```



To use the model, we pass it the input data. This executes the model's `forward`, along with some [background operations](https://github.com/pytorch/pytorch/blob/270111b7b611d174967ed204776985cefca9c144/torch/nn/modules/module.py#L866). Do not call `model.forward()` directly! 모델을 사용하기 위해 입력 데이터를 전달합니다. 이렇게 하면 일부 [백그라운드 작업](https://github.com/pytorch/pytorch/blob/270111b7b611d174967ed204776985cefca9c144/torch/nn/modules/module.py#L866)과 함께 모델의 forward 함수가  실행된다. model.forward()를 부르면 안된다

Calling the model on the input returns a 10-dimensional tensor with raw predicted values for each class. We get the prediction densities by passing it through an instance of the `nn.Softmax` module. 입력에 모델을 호출하면 각 클래스에 대해 원시 예측 값을 갖는 10차원 텐서가 반환됩니다. 우리는 예측 밀도를 nn의 예를 통해 전달한다.Softmax' 모듈.

```python
X = torch.rand(1, 28, 28, device=device)
logits = model(X) 
pred_probab = nn.Softmax(dim=1)(logits)

y_pred = pred_probab.argmax(1)
print(f"Predicted class: {y_pred}")
```

```
Predicted class: tensor([7], device='cuda:0') 
```



## Model layers

Let's break down the layers in the FashionMNIST model. To illustrate it, we will take a sample minibatch of 3 images of size **28x28** and see what happens to it as we pass it through the network.

```python
input_image = torch.rand(3,28,28)
print(input_image.size())
```

```
torch.Size([3, 28, 28]) 
```

### nn.Flatten

We initialize the [nn.Flatten](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html) layer to convert each 2D 28x28 image into a contiguous array of 784 pixel values (the minibatch dimension (at dim=0) is maintained).

```python
flatten = nn.Flatten()
flat_image = flatten(input_image)
print(flat_image.size())
```

```
torch.Size([3, 784]) 
```

### nn.Linear

The [linear layer](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) is a module that applies a linear transformation on the input using it's stored weights and biases.

```python
layer1 = nn.Linear(in_features=28*28, out_features=20)
hidden1 = layer1(flat_image)
print(hidden1.size())
```

```
torch.Size([3, 20]) 
```

### nn.ReLU

Non-linear activations are what create the complex mappings between the model's inputs and outputs. They are applied after linear transformations to introduce *nonlinearity*, helping neural networks learn a wide variety of phenomena.

In this model, we use [nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html) between our linear layers, but there's other activations to introduce non-linearity in your model.

```python
print(f"Before ReLU: {hidden1}\n\n")
hidden1 = nn.ReLU()(hidden1)
print(f"After ReLU: {hidden1}")
```

```markdown
Before ReLU: 
	tensor([[ 2.0324e-01,  5.0068e-04,  8.5407e-02, -5.5120e-01, -4.8736e-01,      
					6.8188e-01,  6.8398e-01,  1.6762e-01,  2.3837e-01,  1.9381e-01,         
					-1.1580e-01, -4.0784e-02, -9.8959e-02,  5.4387e-01, -9.9879e-03,     
          3.5927e-01,  1.2978e-01,  2.5370e-03, -5.2949e-02,  3.7250e-01],     
          
          [ 5.6728e-01, -7.3866e-02, -8.7820e-02, -5.4973e-01, -5.3659e-01,     
          2.6908e-01,  4.4792e-01,  3.0931e-01,  2.7101e-01, -2.0876e-01,        
          -1.6271e-01,  1.0152e-02,  1.2793e-01,  8.1781e-02, -6.8278e-02,    
          5.6277e-01, -2.2239e-01, -1.5318e-01, -2.7621e-02,  2.9807e-01],     
          
          [ 2.7924e-01, -2.2688e-01, -3.0729e-01, -3.6738e-01, -3.9315e-01,     
          4.7412e-01,  4.8254e-01,  3.5839e-02,  2.4099e-01, -3.8630e-01,    
          -1.9694e-03, -4.3444e-04, -6.3402e-03,  4.1470e-01,  3.7856e-02,       
          5.5314e-01, -3.5165e-01, -2.5255e-01,  6.5661e-02,  3.8603e-01]],    
          grad_fn=<AddmmBackward>)  

After ReLU: 
	tensor([[2.0324e-01, 5.0068e-04, 8.5407e-02, 0.0000e+00, 0.0000e+00, 6.8188e-01,     
  				6.8398e-01, 1.6762e-01, 2.3837e-01, 1.9381e-01, 0.0000e+00, 0.0000e+00,       
          0.0000e+00, 5.4387e-01, 0.0000e+00, 3.5927e-01, 1.2978e-01, 2.5370e-03,    
          0.0000e+00, 3.7250e-01],       
          
          [5.6728e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.6908e-01,     
          4.4792e-01, 3.0931e-01, 2.7101e-01, 0.0000e+00, 0.0000e+00, 1.0152e-02,         
          1.2793e-01, 8.1781e-02, 0.0000e+00, 5.6277e-01, 0.0000e+00, 0.0000e+00,  
          0.0000e+00, 2.9807e-01],       
          
          [2.7924e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.7412e-01,     
          4.8254e-01, 3.5839e-02, 2.4099e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,     
          0.0000e+00, 4.1470e-01, 3.7856e-02, 5.5314e-01, 0.0000e+00, 0.0000e+00,     
          6.5661e-02, 3.8603e-01]], grad_fn=<ReluBackward0>) 
```

### nn.Sequential

[nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) is an ordered container of modules. The data is passed through all the modules in the same order as defined. You can use sequential containers to put together a quick network like `seq_modules`.

```python
seq_modules = nn.Sequential(
  flatten,
  layer1,
  nn.ReLU(),
  nn.Linear(20, 10)
)

input_image = torch.rand(3,28,28)
logits = seq_modules(input_image)
```

### nn.Softmax

The last linear layer of the neural network returns `logits` - raw values in [`-infty`, `infty`] - which are passed to the [nn.Softmax](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html) module. The logits are scaled to values [0, 1] representing the model's predicted densities for each class. `dim` parameter indicates the dimension along which the values must sum to 1.

신경망의 마지막 선형 계층은 [nnn]에 전달되는 'logits' - 원시 값(['-infty', 'infty')] 을 반환합니다. [Softmax](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html) 모듈. 로짓은 각 클래스에 대한 모델의 예측 밀도를 나타내는 값 [0, 1]으로 스케일링됩니다. dim 매개 변수는 값의 합계가 1이 되어야 하는 차원을 나타냅니다.

```python
softmax = nn.Softmax(dim=1)
pred_probab = softmax(logits)
```



## Model parameters

Many layers inside a neural network are *parameterized*, i.e. have associated weights and biases that are optimized during training. Subclassing `nn.Module` automatically tracks all fields defined inside your model object, and makes all parameters accessible using your model's `parameters()` or `named_parameters()` methods.

신경 네트워크 내부의 많은 계층은 *파라미터화* 된다. 즉, 훈련 중에 최적화된 관련 가중치와 편향을 갖는다. 하위 분류 'nn.Module'은 모델 개체 내에 정의된 모든 필드를 자동으로 추적하고 모델의 'parameters()' 또는 'nameed_parameters()' 방법을 사용하여 모든 매개 변수에 액세스할 수 있도록 합니다.

In this example, we iterate over each parameter, and print its size and a preview of its values.

```python
print("Model structure: ", model, "\n\n")
for name, param in model.named_parameters():
  print("Layer: {name} | Size: {param.size()} | Values : {param[:2]} \n")
```

```markdown
Model structure: 

NeuralNetwork( 
	(flatten): Flatten(start_dim=1, end_dim=-1)  
	(linear_relu_stack): 
		Sequential(   
    (0): Linear(in_features=784, out_features=512, bias=True) 
    (1): ReLU()  
    (2): Linear(in_features=512, out_features=512, bias=True)   
    (3): ReLU()  
    (4): Linear(in_features=512, out_features=10, bias=True)
    (5): ReLU() 
	) 
)   


Layer: 
	linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : 
		tensor([[ 0.0191, -0.0205, -0.0221,  ...,  0.0056, -0.0007, -0.0028],      
    				[ 0.0178, -0.0342,  0.0295,  ...,  0.0061,  0.0315, -0.0030]],     
            device='cuda:0', grad_fn=<SliceBackward>) 
            
Layer: 
	linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : 
		tensor([ 0.0133, -0.0155], device='cuda:0', grad_fn=<SliceBackward>) 

Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : 
	tensor([[-0.0369,  0.0364,  0.0168,  ..., -0.0021,  0.0277,  0.0301],    
  				[ 0.0075,  0.0374,  0.0043,  ...,  0.0358, -0.0085, -0.0374]],    
          device='cuda:0', grad_fn=<SliceBackward>)  

Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : 
	tensor([-0.0085,  0.0230], device='cuda:0', grad_fn=<SliceBackward>) 

Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : 
	tensor([[ 0.0376, -0.0267, -0.0027,  ...,  0.0063,  0.0413,  0.0334],       
  				[-0.0439, -0.0041,  0.0218,  ...,  0.0198, -0.0212,  0.0009]],   
          device='cuda:0', grad_fn=<SliceBackward>)  

Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : 
	tensor([-0.0120, -0.0102], device='cuda:0', grad_fn=<SliceBackward>)  
```

## Check your knowledge

- 1. The base class for all neural network modules in PyTorch is `torch.nn.Module`
     - True
       - Correct!
     - False

---

# Automatic differentiation

Completed200 XP

- 10 minutes

## Automatic differentiation with `torch.autograd`

When training neural networks, the most frequently used algorithm is **back propagation**. In this algorithm, parameters (model weights) are adjusted according to the **gradient** of the loss function with respect to the given parameter. 신경 네트워크를 교육할 때 가장 자주 사용되는 알고리즘은 **Backpropagation**입니다. 이 알고리즘에서 파라미터(모델 가중치)는 주어진 파라미터에 대해 손실 함수의 **구배**에 따라 조정됩니다.

To compute those gradients, PyTorch has a built-in differentiation engine called `torch.autograd`. It supports automatic computation of gradient for any computational graph. 이러한 구배를 계산하기 위해 PyTorch에는 'Torch'라는 차별화 엔진이 내장되어 있습니다.autograd'. 모든 계산 그래프에 대해 구배 자동 계산을 지원합니다.

Consider the simplest one-layer neural network, with input `x`, parameters `w` and `b`, and some loss function. It can be defined in PyTorch in the following manner: 입력 'x', 매개 변수 'w' 및 'b'가 포함된 가장 단순한 1계층 신경 네트워크와 일부 손실 함수를 고려하십시오. 다음과 같은 방법으로 PyTorch에서 정의할 수 있습니다.

```python
import torch
x = torch.ones(5) # input tensor
y = torch.zeros(3) # expected output
w = torch.randn(5, 3, requires_grad=True)
b = torch.randn(3, requires_grad=True)
z = torch.matmul(x, w)+b

loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)
```



## Tensors, Functions and Computational graph

This code defines the following **computational graph**:

![Diagram showing a computational graph with two parameters 'w' and 'b' to compute the gradients of loss.](https://docs.microsoft.com/en-us/learn/modules/intro-machine-learning-pytorch/images/computational-graph.png)

In this network, `w` and `b` are **parameters**, which we need to optimize. Thus, we need to be able to compute the gradients of loss function with respect to those variables. In order to do that, we set the `requires_grad` property of those tensors.

> **Note:** You can set the value of `requires_grad` when creating a tensor, or later by using `x.requires_grad_(True)` method.

A function that we apply to tensors to construct computational graph is in fact an object of class `Function`. This object knows how to compute the function in the *forward* direction, and also how to compute its derivative during the *backward propagation* step. A reference to the backward propagation function is stored in `grad_fn` property of a tensor. You can find more information of `Function` [in the documentation](https://pytorch.org/docs/stable/autograd.html#function).

```python
print('Gradient function for z =',z.grad_fn)
print('Gradient function for loss =', loss.grad_fn)
```

- Queued

## Computing gradients

To optimize weights of parameters in the neural network, we need to compute the derivatives of our loss function with respect to parameters, namely, we need $\frac{∂loss}{∂w}$ and $\frac{∂loss}{∂b}$ under some fixed values of `x` and `y`. To compute those derivatives, we call `loss.backward()`, and then retrieve the values from `w.grad` and `b.grad`:

```python
loss.backward()
print(w.grad)
print(b.grad)
```

> **Note:** We can only obtain the `grad` properties for the leaf nodes of the computational graph, which have `requires_grad` property set to `True`. For all other nodes in our graph, gradients will not be available. In addition, we can only perform gradient calculations using `backward` once on a given graph, for performance reasons. If we need to do several `backward` calls on the same graph, we need to pass `retain_graph=True` to the `backward` call.

## Disabling gradient tracking

By default, all tensors with `requires_grad=True` are tracking their computational history and support gradient computation. However, there are some cases when we do not need to do that, for example, when we have trained the model and just want to apply it to some input data, i.e. we only want to do *forward* computations through the network. We can stop tracking computations by surrounding our computation code with `torch.no_grad()` block:

기본적으로 'required_grad=True'를 가진 모든 텐서는 계산 이력을 추적하고 구배 계산을 지원합니다. 그러나 모델을 교육한 후 일부 입력 데이터에 적용하려는 경우와 같이 네트워크를 통해 *전달* 연산만 수행하려는 경우가 있습니다. 컴퓨팅 코드를 'torch.no_grade' 블록으로 둘러싸서 컴퓨팅 추적을 중지할 수 있습니다.

```python
z = torch.matmul(x, w)+b
print(z.requires_grad)
with torch.no_grad():
  z = torch.matmul(x, w)+b
print(z.requires_grad)
```

- Another way to achieve the same result is to use the `detach()` method on the tensor:

```python
z = torch.matmul(x, w)+b
z_det = z.detach()
print(z_det.requires_grad)
```

- There are reasons you might want to disable gradient tracking:
  - To mark some parameters in your neural network at **frozen parameters**. This is a very common scenario for [fine tuning a pre-trained network](https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html)
  - To **speed up computations** when you are only doing forward pass, because computations on tensors that do not track gradients would be more efficient.



## More on Computational Graphs

Conceptually, autograd keeps a record of data (tensors) and all executed operations (along with the resulting new tensors) in a directed acyclic graph (DAG) consisting of [Function](https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function) objects. In this DAG, leaves are the input tensors, roots are the output tensors. By tracing this graph from roots to leaves, you can automatically compute the gradients using the chain rule. 

개념적으로, 자동 격자는 데이터(측정기)와 모든 실행된 작업의 기록을 [Function](https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function) 객체로 구성된 지시 순환 그래프(DAG)에 보관한다. 이 DAG에서 리프는 입력 텐서이고 루트는 출력 텐서입니다. 이 그래프를 루트에서 리프까지 추적하면 체인 규칙을 사용하여 구배를 자동으로 계산할 수 있습니다.

In a forward pass, autograd does two things simultaneously:

- run the requested operation to compute a resulting tensor
- maintain the operation’s *gradient function* in the DAG.

The backward pass kicks off when `.backward()` is called on the DAG root. `autograd` then:

- computes the gradients from each `.grad_fn`,
- accumulates them in the respective tensor’s `.grad` attribute
- using the chain rule, propagates all the way to the leaf tensors.



**DAGs are dynamic in PyTorch**

An important thing to note is that the graph is recreated from scratch; after each `.backward()` call, autograd starts populating a new graph. This is exactly what allows you to use control flow statements in your model; you can change the shape, size and operations at every iteration if needed.

중요한 점은 그래프가 처음부터 다시 생성된다는 것입니다. 각 '.backward()' 호출 후 autograd가 새 그래프를 채우기 시작합니다. 이 기능을 사용하면 모델에서 제어 흐름 문을 사용할 수 있습니다. 필요한 경우 반복할 때마다 모양, 크기 및 작업을 변경할 수 있습니다.



## Optional reading: Tensor gradients and Jacobian products

In many cases, we have a scalar loss function, and we need to compute the gradient with respect to some parameters. However, there are cases when the output function is an arbitrary tensor. In this case, PyTorch allows you to compute so-called **Jacobian product**, and not the actual gradient.

For a vector function →y=f(→x)y→=f(x→), where →x=⟨x1,…,xn⟩x→=⟨x1,…,xn⟩ and →y=⟨y1,…,ym⟩y→=⟨y1,…,ym⟩, a gradient of →yy→ with respect to →xx→ is given by **Jacobian matrix**:

\begin{align}\begin{align}J=\left(\begin{array}{ccc} \frac{\partial y{1}}{\partial x{1}} & \cdots & \frac{\partial y{1}}{\partial x{n}}\ \vdots & \ddots & \vdots\ \frac{\partial y{m}}{\partial x{1}} & \cdots & \frac{\partial y{m}}{\partial x{n}} \end{array}\right)\end{align}\end{align}

Instead of computing the Jacobian matrix itself, PyTorch allows you to compute **Jacobian Product** vT⋅JvT⋅J for a given input vector v=(v1…vm)v=(v1…vm). This is achieved by calling `backward` with vv as an argument. The size of vv should be the same as the size of the original tensor, with respect to which we want to compute the product:

```python
inp = torch.eye(5, requires_grad=True)
out = (inp+1).pow(2)
out.backward(torch.ones_like(inp), retain_graph=True)
print("First call\n", inp.grad)

out.backward(torch.ones_like(inp), retain_graph=True)
print("\nSecond call\n", inp.grad)

inp.grad.zero_()
out.backward(torch.ones_like(inp), retain_graph=True)
print("\nCall after zeroing gradients\n", inp.grad)
```

- Queued

Notice that when we call `backward` for the second time with the same argument, the value of the gradient is different. This happens because when doing `backward` propagation, PyTorch **accumulates the gradients**, i.e. the value of computed gradients is added to the `grad` property of all leaf nodes of computational graph. If you want to compute the proper gradients, you need to zero out the `grad` property before. In real-life training an *optimizer* helps us to do this.

같은 주장으로 두 번째 '뒤로'라고 부르면 구배 값이 달라진다는 점에 유의하십시오. 이는 PyTorch가 '뒤로' 전파를 수행할 때 ** 구배*를 누적하기 때문에 발생합니다. 즉, 계산된 구배 값이 계산 그래프의 모든 리프 노드의 '그라드' 속성에 추가됩니다. 적절한 구배를 계산하려면 먼저 grad 속성을 0으로 설정해야 합니다. 실제 교육에서 *optimizer*는 이를 수행하는 데 도움이 됩니다.

> **Note:** previously we were calling `backward()` function without parameters. This is equivalent to calling `backward(torch.tensor(1.0))`, which is a useful way to compute the gradients in case of a scalar-valued function, such as loss during neural network training.

## Check your knowledge

1. The purpose for the `torch.autograd` engine is to:
   - automatically grade a model's accuracy
   - automatically optimize a model's internal layer structure
   - automatically optimize the set of data used to build a model
   - automatically compute gradients during model optimization
     - Correct!

------

# Learn about the optimization loop

200 XP

- 15 minutes

```

```

%matplotlib inline

**

**

**

**

**

**Code**Markdown

# Optimizing the model parameters

Now that we have a model and data it's time to train, validate and test our model by optimizing its parameters on our data. Training a model is an iterative process; in each iteration (called an *epoch*) the model makes a guess about the output, calculates the error in its guess (*loss*), collects the derivatives of the error with respect to its parameters (as we saw in the module), and **optimizes** these parameters using gradient descent. For a more detailed walkthrough of this process, check out this video on [backpropagation from 3Blue1Brown](https://www.youtube.com/watch?v=tIeHLnjs5U8).

## Prerequisite code

We load the code from the previous modules on **Datasets & DataLoaders** and **Build Model**

```

```



import torch

from torch import nn

from torch.utils.data import DataLoader

from torchvision import datasets

from torchvision.transforms import ToTensor, Lambda

training_data = datasets.FashionMNIST(

  root="data",

  train=True,

  download=True,

  transform=ToTensor()

)

test_data = datasets.FashionMNIST(

  root="data",

  train=False,

  download=True,

  transform=ToTensor()

)

train_dataloader = DataLoader(training_data, batch_size=64)

test_dataloader = DataLoader(test_data, batch_size=64)

class NeuralNetwork(nn.Module):

  def __init__(self):

​    super(NeuralNetwork, self).__init__()

​    self.flatten = nn.Flatten()

​    self.linear_relu_stack = nn.Sequential(

​      nn.Linear(28*28, 512),

​      nn.ReLU(),

​      nn.Linear(512, 512),

​      nn.ReLU(),

​      nn.Linear(512, 10),

​      nn.ReLU()

​    )

  def forward(self, x):

​    x = self.flatten(x)

​    logits = self.linear_relu_stack(x)

​    return logits

model = NeuralNetwork()



## Setting hyperparameters

Hyperparameters are adjustable parameters that let you control the model optimization process. Different hyperparameter values can impact model training and convergence rates ([read more](https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html) about hyperparameter tuning)

We define the following hyperparameters for training:

- **Number of Epochs** - the number times to iterate over the dataset
- **Batch Size** - the number of data samples seen by the model in each epoch
- **Learning Rate** - how much to update models parameters at each batch/epoch. Smaller values yield slow learning speed, while large values may result in unpredictable behavior during training.

```

```



learning_rate = 1e-3

batch_size = 64

epochs = 5

**

## Add an optimization loop

Once we set our hyperparameters, we can then train and optimize our model with an optimization loop. Each iteration of the optimization loop is called an **epoch**.

Each epoch consists of two main parts:

- **The Train Loop** - iterate over the training dataset and try to converge to optimal parameters.
- **The Validation/Test Loop** - iterate over the test dataset to check if model performance is improving.

Let's briefly familiarize ourselves with some of the concepts used in the training loop. Jump ahead to see the `full-impl-label` of the optimization loop.

### Add a loss function

When presented with some training data, our untrained network is likely not to give the correct answer. **Loss function** measures the degree of dissimilarity of obtained result to the target value, and it is the loss function that we want to minimize during training. To calculate the loss we make a prediction using the inputs of our given data sample and compare it against the true data label value.

Common loss functions include [nn.MSELoss](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss) (Mean Square Error) for regression tasks, and [nn.NLLLoss](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss) (Negative Log Likelihood) for classification. [nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss) combines `nn.LogSoftmax` and `nn.NLLLoss`.

We pass our model's output logits to `nn.CrossEntropyLoss`, which will normalize the logits and compute the prediction error.

```

```



\# Initialize the loss function

loss_fn = nn.CrossEntropyLoss()

**

### Optimization pass

Optimization is the process of adjusting model parameters to reduce model error in each training step. **Optimization algorithms** define how this process is performed (in this example we use Stochastic Gradient Descent). All optimization logic is encapsulated in the `optimizer` object. Here, we use the SGD optimizer; additionally, there are many [different optimizers](https://pytorch.org/docs/stable/optim.html) available in PyTorch such as ADAM and RMSProp, that work better for different kinds of models and data.

We initialize the optimizer by registering the model's parameters that need to be trained, and passing in the learning rate hyperparameter.

```

```



optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)



Inside the training loop, optimization happens in three steps:

- Call `optimizer.zero_grad()` to reset the gradients of model parameters. Gradients by default add up; to prevent double-counting, we explicitly zero them at each iteration.
- Back-propagate the prediction loss with a call to `loss.backwards()`. PyTorch deposits the gradients of the loss w.r.t. each parameter.
- Once we have our gradients, we call `optimizer.step()` to adjust the parameters by the gradients collected in the backward pass.

## Full implementation

We define `train_loop` that loops over our optimization code, and `test_loop` that evaluates the model's performance against our test data.

```python
def train_loop(dataloader, model, loss_fn, optimizer):
  size = len(dataloader.dataset)
  for batch, (X, y) in enumerate(dataloader):    
    # Compute prediction and loss
    pred = model(X)
    loss = loss_fn(pred, y) 
    # Backpropagation
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    if batch % 100 == 0:
      loss, current = loss.item(), batch * len(X)
      print(f"loss: {loss:>7f} [{current:>5d}/{size:>5d}]")

def test_loop(dataloader, model, loss_fn):
  size = len(dataloader.dataset)
  test_loss, correct = 0, 0
  with torch.no_grad():
    for X, y in dataloader:
      pred = model(X)
      test_loss += loss_fn(pred, y).item()
      correct += (pred.argmax(1) == y).type(torch.float).sum().item()
      
  test_loss /= size
  correct /= size
  print(f"Test Error: \n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \n")
```



We initialize the loss function and optimizer, and pass it to `train_loop` and `test_loop`. Feel free to increase the number of epochs to track the model's improving performance.

```python
loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)
epochs = 10
for t in range(epochs):
  print("Epoch {t+1}\n-------------------------------")
  train_loop(train_dataloader, model, loss_fn, optimizer)
  test_loop(test_dataloader, model, loss_fn)
print("Done!")
```



```
Epoch 1 ------------------------------- loss: 2.307260  [    0/60000] loss: 2.305284  [ 6400/60000] loss: 2.293966  [12800/60000] loss: 2.291592  [19200/60000] loss: 2.288022  [25600/60000] loss: 2.259277  [32000/60000] loss: 2.277950  [38400/60000] loss: 2.252569  [44800/60000] loss: 2.238333  [51200/60000] loss: 2.239141  [57600/60000] Test Error:  Accuracy: 27.5%, Avg loss: 0.035050  

Epoch 2 ------------------------------- loss: 2.222609  [    0/60000] loss: 2.244805  [ 6400/60000] loss: 2.209550  [12800/60000] loss: 2.227453  [19200/60000] loss: 2.217051  [25600/60000] loss: 2.162092  [32000/60000] loss: 2.206926  [38400/60000] loss: 2.151579  [44800/60000] loss: 2.117667  [51200/60000] loss: 2.143689  [57600/60000] Test Error:  Accuracy: 38.9%, Avg loss: 0.033368 

Epoch 3 ------------------------------- loss: 2.102783  [    0/60000] loss: 2.154025  [ 6400/60000] loss: 2.076486  [12800/60000] loss: 2.124048  [19200/60000] loss: 2.107713  [25600/60000] loss: 2.014179  [32000/60000] loss: 2.090220  [38400/60000] loss: 1.989485  [44800/60000] loss: 1.933911  [51200/60000] loss: 2.002917  [57600/60000] Test Error:  Accuracy: 41.2%, Avg loss: 0.030885  

Epoch 4 ------------------------------- loss: 1.926293  [    0/60000] loss: 2.019496  [ 6400/60000] loss: 1.888668  [12800/60000] loss: 1.987653  [19200/60000] loss: 1.968171  [25600/60000] loss: 1.838344  [32000/60000] loss: 1.951870  [38400/60000] loss: 1.808960  [44800/60000] loss: 1.749038  [51200/60000] loss: 1.868777  [57600/60000] Test Error:  Accuracy: 44.4%, Avg loss: 0.028537 

Epoch 5 ------------------------------- loss: 1.754023  [    0/60000] loss: 1.889865  [ 6400/60000] loss: 1.724985  [12800/60000] loss: 1.880932  [19200/60000] loss: 1.852289  [25600/60000] loss: 1.703095  [32000/60000] loss: 1.850078  [38400/60000] loss: 1.679640  [44800/60000] loss: 1.618462  [51200/60000] loss: 1.781099  [57600/60000] Test Error:  Accuracy: 46.4%, Avg loss: 0.026904 

Epoch 6 ------------------------------- loss: 1.629323  [    0/60000] loss: 1.794621  [ 6400/60000] loss: 1.609603  [12800/60000] loss: 1.806047  [19200/60000] loss: 1.771073  [25600/60000] loss: 1.610854  [32000/60000] loss: 1.782800  [38400/60000] loss: 1.593032  [44800/60000] loss: 1.530435  [51200/60000] loss: 1.721836  [57600/60000] Test Error:  Accuracy: 47.5%, Avg loss: 0.025738  

Epoch 7 ------------------------------- loss: 1.541017  [    0/60000] loss: 1.723998  [ 6400/60000] loss: 1.525540  [12800/60000] loss: 1.745950  [19200/60000] loss: 1.714844  [25600/60000] loss: 1.542636  [32000/60000] loss: 1.735072  [38400/60000] loss: 1.529822  [44800/60000] loss: 1.467118  [51200/60000] loss: 1.675812  [57600/60000] Test Error:  Accuracy: 48.3%, Avg loss: 0.024844  

Epoch 8 ------------------------------- loss: 1.474333  [    0/60000] loss: 1.669000  [ 6400/60000] loss: 1.460421  [12800/60000] loss: 1.694097  [19200/60000] loss: 1.674764  [25600/60000] loss: 1.487773  [32000/60000] loss: 1.699166  [38400/60000] loss: 1.481064  [44800/60000] loss: 1.419311  [51200/60000] loss: 1.638599  [57600/60000] Test Error:  Accuracy: 48.7%, Avg loss: 0.024137 

Epoch 9 ------------------------------- loss: 1.420322  [    0/60000] loss: 1.625176  [ 6400/60000] loss: 1.408073  [12800/60000] loss: 1.649715  [19200/60000] loss: 1.644693  [25600/60000] loss: 1.443653  [32000/60000] loss: 1.671596  [38400/60000] loss: 1.443777  [44800/60000] loss: 1.382555  [51200/60000] loss: 1.608089  [57600/60000] Test Error:  Accuracy: 49.1%, Avg loss: 0.023570  

Epoch 10 ------------------------------- loss: 1.375013  [    0/60000] loss: 1.588062  [ 6400/60000] loss: 1.364595  [12800/60000] loss: 1.612044  [19200/60000] loss: 1.621220  [25600/60000] loss: 1.407904  [32000/60000] loss: 1.649211  [38400/60000] loss: 1.415225  [44800/60000] loss: 1.353849  [51200/60000] loss: 1.582835  [57600/60000] Test Error:  Accuracy: 49.5%, Avg loss: 0.023104  Done! 
```

You may have noticed that the model is initially not very good (that's OK!). Try running the loop for more `epochs` or adjusting the `learning_rate` to a bigger number. It might also be the case that the model configuration we chose might not be the optimal one for this kind of problem (it isn't). Later courses will delve more into the model shapes that work for vision problems.



## Check your knowledge

\1. 

What is the purpose of the Loss Function during model optimization?



The Loss Function measures the degree of dissimilarity of an obtained result to the target value



The loss function's gradient helps the optimizer with appropriate parameter adjustments during training



The loss function is what is minimized during training



- All of the above are true

# Save, load, and run model predictions

```

```



%matplotlib inline

**

**

**

**

**12초

**

```
Matplotlib is building the font cache; this may take a moment. 
```

**Code**Markdown

# Save and load the model

In this unit we will look at how to persist model state with saving, loading and running model predictions.

```

```



import torch

import torch.onnx as onnx

import torchvision.models as models

**9초

## Saving and loading model weights

PyTorch models store the learned parameters in an internal state dictionary, called `state_dict`. These can be persisted via the `torch.save` method:

```

```



model = models.vgg16(pretrained=True)

torch.save(model.state_dict(), 'data/model_weights.pth')

**<1초

```
Downloading: "https://download.pytorch.org/models/vgg16-397923af.pth" to /home/mslearnuser/.cache/torch/hub/checkpoints/vgg16-397923af.pth 
```

100%

528M/528M [00:02<00:00, 251MB/s]

To load model weights, you need to create an instance of the same model first, and then load the parameters using the `load_state_dict()` method.

[4]

model = models.vgg16() # we do not specify pretrained=True, i.e. do not load default weights

model.load_state_dict(torch.load('data/model_weights.pth'))

model.eval()

**3초

```
VGG(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU(inplace=True)
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (6): ReLU(inplace=True)
    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (8): ReLU(inplace=True)
    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (11): ReLU(inplace=True)
    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (13): ReLU(inplace=True)
    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (15): ReLU(inplace=True)
    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (18): ReLU(inplace=True)
    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (20): ReLU(inplace=True)
    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (22): ReLU(inplace=True)
    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (25): ReLU(inplace=True)
    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (27): ReLU(inplace=True)
    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (29): ReLU(inplace=True)
    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))
  (classifier): Sequential(
    (0): Linear(in_features=25088, out_features=4096, bias=True)
    (1): ReLU(inplace=True)
    (2): Dropout(p=0.5, inplace=False)
    (3): Linear(in_features=4096, out_features=4096, bias=True)
    (4): ReLU(inplace=True)
    (5): Dropout(p=0.5, inplace=False)
    (6): Linear(in_features=4096, out_features=1000, bias=True)
  )
)
```

> **Note:** Be sure to call `model.eval()` method before inferencing to set the dropout and batch normalization layers to evaluation mode. Failing to do this will yield inconsistent inference results.

## Saving and loading models with shapes

When loading model weights, we needed to instantiate the model class first, because the class defines the structure of a network. We might want to save the structure of this class together with the model, in which case we can pass `model` (and not `model.state_dict()`) to the saving function:

```

```



torch.save(model, 'data/vgg_model.pth')

**<1초

We can then load the model like this:

```

```



model = torch.load('data/vgg_model.pth')

**<1초

> **Note:** This approach uses Python [pickle](https://docs.python.org/3/library/pickle.html) module when serializing the model, thus it relies on the actual class definition to be available when loading the model.

## Exporting the model to ONNX

PyTorch also has native ONNX export support. Given the dynamic nature of the PyTorch execution graph, however, the export process must traverse the execution graph to produce a persisted ONNX model. For this reason, a test variable of the appropriate size should be passed in to the export routine (in our case, we will create a dummy zero tensor of the correct size):

```

```



input_image = torch.zeros((1,3,224,224))

onnx.export(model, input_image, 'data/model.onnx')

**2초

There are a lot of things you can do with ONNX model, including running inference on different platforms and in different programming languages. For more details, we recommend visiting [ONNX tutorial](https://github.com/onnx/tutorials).

Congratulations! You have completed the PyTorch beginner tutorial! We hope this tutorial has helped you get started with deep learning on PyTorch.

## Check your knowledge

1. What is a PyTorch model `state_dict`?
   - It is a model's internal state dictionary that stores its current accuracy and loss values.
   - It is a model's internal state dictionary that stores versions of the data used for training.
   - It is a model's internal state dictionary that stores its internal layers.
   - It is a model's internal state dictionary that stores learned parameters.
     - Correct!

# The full model building process

```python
%matplotlib inline
```

This unit runs through the API for common tasks in machine learning. Refer to the links in each section to dive deeper.

## Working with data

PyTorch has two [primitives to work with data](https://pytorch.org/docs/stable/data.html): `torch.utils.data.DataLoader` and `torch.utils.data.Dataset`. `Dataset` stores the samples and their corresponding labels, and `DataLoader` wraps an iterable around the `Dataset`.

```python
import torch
from torch import nn
from torch.utils.data import DataLoader
from torchvision import datasets
from torchvision.transforms import ToTensor, Lambda, Compose
import matplotlib.pyplot as plt
```

PyTorch offers domain-specific libraries such as [TorchText](https://pytorch.org/text/stable/index.html), [TorchVision](https://pytorch.org/vision/stable/index.html), and [TorchAudio](https://pytorch.org/audio/stable/index.html), all of which include datasets. For this tutorial, we will be using a TorchVision dataset.

The `torchvision.datasets` module contains `Dataset` objects for many real-world vision data like CIFAR, COCO ([full list here](https://pytorch.org/docs/stable/torchvision/datasets.html)). In this tutorial, we'll use the **FashionMNIST** dataset. Every TorchVision `Dataset` includes two arguments: `transform` and `target_transform` to modify the samples and labels respectively.

```python
# Download training data from open datasets.
training_data = datasets.FashionMNIST(
  root="data",
  train=True,
  download=True,
  transform=ToTensor(),
)

# Download test data from open datasets.
test_data = datasets.FashionMNIST(
  root="data",
  train=False,
  download=True,
  transform=ToTensor(),
)
```

```
Using downloaded and verified file: data/FashionMNIST/raw/train-images-idx3-ubyte.gz Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw Using downloaded and verified file: data/FashionMNIST/raw/train-labels-idx1-ubyte.gz Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw Using downloaded and verified file: data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw Using downloaded and verified file: data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw Processing... Done! ``/anaconda/envs/py37_pytorch/lib/python3.7/site-packages/torchvision/datasets/mnist.py:480: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1607370141920/work/torch/csrc/utils/tensor_numpy.cpp:141.)  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s) 
```

We pass the `Dataset` as an argument to `DataLoader`. This wraps an iterable over our dataset, and supports automatic batching, sampling, shuffling and multiprocess data loading. Here we define a batch size of 64, i.e. each element in the dataloader iterable will return a batch of 64 features and labels.

```python
batch_size = 64
# Create data loaders.
train_dataloader = DataLoader(training_data, batch_size=batch_size)
test_dataloader = DataLoader(test_data, batch_size=batch_size)
for X, y in test_dataloader:
  print("Shape of X [N, C, H, W]: ", X.shape)
  print("Shape of y: ", y.shape, y.dtype)
  break
```

```
Shape of X [N, C, H, W]:  torch.Size([64, 1, 28, 28]) Shape of y:  torch.Size([64]) torch.int64 
```

## Creating models

To define a neural network in PyTorch, we create a class that inherits from [nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). We define the layers of the network in the `__init__` function and specify how data will pass through the network in the `forward` function. To accelerate operations in the neural network, we move it to the GPU if available.

```python
# Get cpu or gpu device for training.
device = "cuda" if torch.cuda.is_available() else "cpu"
print("Using {} device".format(device))
# Define model
class NeuralNetwork(nn.Module):
  def __init__(self):
    super(NeuralNetwork, self).__init__()
    self.flatten = nn.Flatten()
    self.linear_relu_stack = nn.Sequential(
      nn.Linear(28*28, 512),
      nn.ReLU(),
      nn.Linear(512, 512),
      nn.ReLU(),
      nn.Linear(512, 10),
      nn.ReLU()
    )
    
 def forward(self, x):
    x = self.flatten(x)
    logits = self.linear_relu_stack(x)
    return logits
  
model = NeuralNetwork().to(device)
print(model)
```



```markdown
Using cuda device NeuralNetwork( 
	(flatten): Flatten(start_dim=1, end_dim=-1)  (linear_relu_stack): 
	Sequential(
    (0): Linear(in_features=784, out_features=512, bias=True)   
    (1): ReLU()   
    (2): Linear(in_features=512, out_features=512, bias=True)   
    (3): ReLU()    
    (4): Linear(in_features=512, out_features=10, bias=True)  
    (5): ReLU()  	
	)
) 
```

## Optimizing the Model Parameters

To train a model, we need a [loss function](https://pytorch.org/docs/stable/nn.html#loss-functions>) and an [optimizer](https://pytorch.org/docs/stable/optim.html).

```python
loss_fn = nn.CrossEntropyLoss()
learning_rate = 1e-3
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)
```

In a single training loop, the model makes predictions on the training dataset (fed to it in batches), and back-propagates the prediction error to adjust the model's parameters.

```python
def train(dataloader, model, loss_fn, optimizer):
  size = len(dataloader.dataset)
  for batch, (X, y) in enumerate(dataloader):
    X, y = X.to(device), y.to(device)
    # Compute prediction error
    pred = model(X)
    loss = loss_fn(pred, y)
    
    # Backpropagation
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    if batch % 100 == 0:
      loss, current = loss.item(), batch * len(X)
      print(f"loss: {loss:>7f} [{current:>5d}/{size:>5d}]")
```

We can also check the model's performance against the test dataset to ensure it is learning.

```python
def test(dataloader, model):
  size = len(dataloader.dataset)
  model.eval()
  test_loss, correct = 0, 0
  
  with torch.no_grad():
    for X, y in dataloader:
      X, y = X.to(device), y.to(device)
      pred = model(X)
      test_loss += loss_fn(pred, y).item()
      correct += (pred.argmax(1) == y).type(torch.float).sum().item()
      test_loss /= size

  correct /= size
  print("Test Error: \n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \n")
```

The training process is conducted over several iterations (*epochs*). During each epoch, the model learns parameters to make better predictions. We print the model's accuracy and loss at each epoch; we'd like to see the accuracy increase and the loss decrease with every epoch.

```python
epochs = 15
for t in range(epochs):
  print(f"Epoch {t+1}\n-------------------------------")
  train(train_dataloader, model, loss_fn, optimizer)
  test(test_dataloader, model)
print("Done!")
```

```
Epoch 1 ------------------------------- loss: 2.308244  [    0/60000] loss: 2.301662  [ 6400/60000] loss: 2.299381  [12800/60000] loss: 2.293565  [19200/60000] loss: 2.278485  [25600/60000] loss: 2.281788  [32000/60000] loss: 2.278154  [38400/60000] loss: 2.264884  [44800/60000] loss: 2.278050  [51200/60000] loss: 2.265731  [57600/60000] Test Error:  Accuracy: 33.0%, Avg loss: 0.035361  
Epoch 2 ------------------------------- loss: 2.263529  [    0/60000] loss: 2.243206  [ 6400/60000] loss: 2.234885  [12800/60000] loss: 2.242897  [19200/60000] loss: 2.183212  [25600/60000] loss: 2.204664  [32000/60000] loss: 2.196060  [38400/60000] loss: 2.165953  [44800/60000] loss: 2.213932  [51200/60000] loss: 2.189297  [57600/60000] Test Error:  Accuracy: 44.4%, Avg loss: 0.033856  
Epoch 3 ------------------------------- loss: 2.191511  [    0/60000] loss: 2.151078  [ 6400/60000] loss: 2.128932  [12800/60000] loss: 2.153729  [19200/60000] loss: 2.017538  [25600/60000] loss: 2.086752  [32000/60000] loss: 2.070010  [38400/60000] loss: 2.020661  [44800/60000] loss: 2.123916  [51200/60000] loss: 2.086038  [57600/60000] Test Error:  Accuracy: 45.5%, Avg loss: 0.031705  
Epoch 4 ------------------------------- loss: 2.092045  [    0/60000] loss: 2.025146  [ 6400/60000] loss: 1.990673  [12800/60000] loss: 2.033067  [19200/60000] loss: 1.811971  [25600/60000] loss: 1.952929  [32000/60000] loss: 1.915350  [38400/60000] loss: 1.853108  [44800/60000] loss: 2.003140  [51200/60000] loss: 1.930078  [57600/60000] Test Error:  Accuracy: 46.9%, Avg loss: 0.029158  
Epoch 5 ------------------------------- loss: 1.977102  [    0/60000] loss: 1.892723  [ 6400/60000] loss: 1.856888  [12800/60000] loss: 1.868795  [19200/60000] loss: 1.605191  [25600/60000] loss: 1.815202  [32000/60000] loss: 1.708351  [38400/60000] loss: 1.687288  [44800/60000] loss: 1.848937  [51200/60000] loss: 1.731334  [57600/60000] Test Error:  Accuracy: 52.0%, Avg loss: 0.026580  
Epoch 6 ------------------------------- loss: 1.853199  [    0/60000] loss: 1.764611  [ 6400/60000] loss: 1.737372  [12800/60000] loss: 1.711567  [19200/60000] loss: 1.431005  [25600/60000] loss: 1.696275  [32000/60000] loss: 1.523592  [38400/60000] loss: 1.551817  [44800/60000] loss: 1.700327  [51200/60000] loss: 1.556069  [57600/60000] Test Error:  Accuracy: 54.7%, Avg loss: 0.024369 
Epoch 7 ------------------------------- loss: 1.732549  [    0/60000] loss: 1.649983  [ 6400/60000] loss: 1.629636  [12800/60000] loss: 1.581321  [19200/60000] loss: 1.292695  [25600/60000] loss: 1.598631  [32000/60000] loss: 1.379164  [38400/60000] loss: 1.447635  [44800/60000] loss: 1.583671  [51200/60000] loss: 1.433749  [57600/60000] Test Error:  Accuracy: 55.2%, Avg loss: 0.022687 
Epoch 8 ------------------------------- loss: 1.632018  [    0/60000] loss: 1.558001  [ 6400/60000] loss: 1.538675  [12800/60000] loss: 1.487822  [19200/60000] loss: 1.193841  [25600/60000] loss: 1.522596  [32000/60000] loss: 1.279208  [38400/60000] loss: 1.372921  [44800/60000] loss: 1.502596  [51200/60000] loss: 1.354262  [57600/60000] Test Error:  Accuracy: 55.8%, Avg loss: 0.021514 
Epoch 9 ------------------------------- loss: 1.555871  [    0/60000] loss: 1.491291  [ 6400/60000] loss: 1.469144  [12800/60000] loss: 1.425747  [19200/60000] loss: 1.128848  [25600/60000] loss: 1.467914  [32000/60000] loss: 1.212620  [38400/60000] loss: 1.320562  [44800/60000] loss: 1.446733  [51200/60000] loss: 1.302307  [57600/60000] Test Error:  Accuracy: 56.7%, Avg loss: 0.020715  
Epoch 10 ------------------------------- loss: 1.500138  [    0/60000] loss: 1.444472  [ 6400/60000] loss: 1.417692  [12800/60000] loss: 1.384599  [19200/60000] loss: 1.086298  [25600/60000] loss: 1.428751  [32000/60000] loss: 1.166260  [38400/60000] loss: 1.283096  [44800/60000] loss: 1.406363  [51200/60000] loss: 1.266232  [57600/60000] Test Error:  Accuracy: 57.6%, Avg loss: 0.020150  
Epoch 11 ------------------------------- loss: 1.458530  [    0/60000] loss: 1.410509  [ 6400/60000] loss: 1.379321  [12800/60000] loss: 1.355739  [19200/60000] loss: 1.056411  [25600/60000] loss: 1.399136  [32000/60000] loss: 1.132577  [38400/60000] loss: 1.255147  [44800/60000] loss: 1.375376  [51200/60000] loss: 1.240256  [57600/60000] Test Error:  Accuracy: 58.4%, Avg loss: 0.019722 
Epoch 12 ------------------------------- loss: 1.425603  [    0/60000] loss: 1.383455  [ 6400/60000] loss: 1.349400  [12800/60000] loss: 1.334092  [19200/60000] loss: 1.034445  [25600/60000] loss: 1.376258  [32000/60000] loss: 1.106107  [38400/60000] loss: 1.232482  [44800/60000] loss: 1.349747  [51200/60000] loss: 1.219756  [57600/60000] Test Error:  Accuracy: 59.2%, Avg loss: 0.019377  
Epoch 13 ------------------------------- loss: 1.397835  [    0/60000] loss: 1.360644  [ 6400/60000] loss: 1.324387  [12800/60000] loss: 1.316506  [19200/60000] loss: 1.017424  [25600/60000] loss: 1.357446  [32000/60000] loss: 1.081942  [38400/60000] loss: 1.213508  [44800/60000] loss: 1.327295  [51200/60000] loss: 1.202648  [57600/60000] Test Error:  Accuracy: 60.1%, Avg loss: 0.019083 
Epoch 14 ------------------------------- loss: 1.373411  [    0/60000] loss: 1.340207  [ 6400/60000] loss: 1.303139  [12800/60000] loss: 1.301876  [19200/60000] loss: 1.002764  [25600/60000] loss: 1.342115  [32000/60000] loss: 1.060629  [38400/60000] loss: 1.196334  [44800/60000] loss: 1.306820  [51200/60000] loss: 1.187618  [57600/60000] Test Error:  Accuracy: 60.8%, Avg loss: 0.018823  
Epoch 15 ------------------------------- loss: 1.351294  [    0/60000] loss: 1.321608  [ 6400/60000] loss: 1.284594  [12800/60000] loss: 1.289170  [19200/60000] loss: 0.990012  [25600/60000] loss: 1.327834  [32000/60000] loss: 1.041283  [38400/60000] loss: 1.181126  [44800/60000] loss: 1.287974  [51200/60000] loss: 1.174571  [57600/60000] Test Error:  Accuracy: 61.2%, Avg loss: 0.018589  Done! 
```

The accuracy will initially not be very good (that's OK!). Try running the loop for more `epochs` or adjusting the `learning_rate` to a bigger number. It might also be the case that the model configuration we chose might not be the optimal one for this kind of problem (it isn't). Later courses will delve more into the model shapes that work for vision problems.

## Saving Models

A common way to save a model is to serialize the internal state dictionary (containing the model parameters).

```python
torch.save(model.state_dict(), "data/model.pth")
print("Saved PyTorch Model State to model.pth")
```



```
Saved PyTorch Model State to model.pth 
```

## Loading Models

The process for loading a model includes re-creating the model structure and loading the state dictionary into it.

```python
model = NeuralNetwork()
model.load_state_dict(torch.load("data/model.pth"))
```



```
<All keys matched successfully>
```

This model can now be used to make predictions.

```python
classes = [
  "T-shirt/top",
  "Trouser",
  "Pullover",
  "Dress",
  "Coat",
  "Sandal",
  "Shirt",
  "Sneaker",
  "Bag",
  "Ankle boot",
]

model.eval()
x, y = test_data[0][0], test_data[0][1]
with torch.no_grad():
  pred = model(x)
  predicted, actual = classes[pred[0].argmax(0)], classes[y]
  print(f'Predicted: "{predicted}", Actual: "{actual}"')
```

```
Predicted: "Ankle boot", Actual: "Ankle boot" 
```

---

# Summary

- 1 minute

In this module we introduced the key concepts to building machine learning models and implemented those concepts with PyTorch. We built a Computer Vision model that could classify images of T-shirt/top, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, and Ankle boots. Now that you learned some of the basics of machine learning concepts, keep working through the Microsoft Learn content to learn more about building different types of machine learning models with PyTorch.



