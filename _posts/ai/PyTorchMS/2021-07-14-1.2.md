---
layout: article
title: Introduction To PyTorch (2)
tags: PyTorch AI
category: PyTorch AI
picture_frame: shadow

---

**[Introduction to PyTorch](https://docs.microsoft.com/en-us/learn/modules/intro-machine-learning-pytorch/)**  
by [PyTorch Fundamentals @ Mircosoft Learn](https://docs.microsoft.com/en-us/learn/paths/pytorch-fundamentals/)
<!--more-->

## TOC

- [Introduction](#introduction)
- [What are Tensors?](#what-are-tensors)
- [Load data with PyTorch Datasets and DataLoaders](#load-data-with-pytorch-datasets-and-dataloaders)
- [Transform the data](#transform-the-data)
- [Building the model layers](#building-the-model-layers)
- [Automatic differentiation](#automatic-differentation)
- [Learn about the optimization loop](#learn-about-the-optimization-loop)
- [Save, load, and run model predictions](#save-load-and-run-model-predictions)
- [The full model building process](#the-full-model-building-process)
- [Summary](#summary)

## Automatic differentiation

Completed200 XP

- 10 minutes

### Automatic differentiation with `torch.autograd`

When training neural networks, the most frequently used algorithm is **back propagation**. In this algorithm, parameters (model weights) are adjusted according to the **gradient** of the loss function with respect to the given parameter. 신경 네트워크를 교육할 때 가장 자주 사용되는 알고리즘은 **Backpropagation**입니다. 이 알고리즘에서 파라미터(모델 가중치)는 주어진 파라미터에 대해 손실 함수의 **구배**에 따라 조정됩니다.

To compute those gradients, PyTorch has a built-in differentiation engine called `torch.autograd`. It supports automatic computation of gradient for any computational graph. 이러한 구배를 계산하기 위해 PyTorch에는 'Torch'라는 차별화 엔진이 내장되어 있습니다.autograd'. 모든 계산 그래프에 대해 구배 자동 계산을 지원한다.

Consider the simplest one-layer neural network, with input `x`, parameters `w` and `b`, and some loss function. It can be defined in PyTorch in the following manner: 입력 'x', 매개 변수 'w' 및 'b'가 포함된 가장 단순한 1계층 신경 네트워크와 일부 손실 함수를 고려하십시오. 다음과 같은 방법으로 PyTorch에서 정의할 수 있습니다.

```python
import torch
x = torch.ones(5) # input tensor
y = torch.zeros(3) # expected output
w = torch.randn(5, 3, requires_grad=True)
b = torch.randn(3, requires_grad=True)
z = torch.matmul(x, w)+b

loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)
```



### Tensors, Functions and Computational graph

This code defines the following **computational graph**:

![Diagram showing a computational graph with two parameters 'w' and 'b' to compute the gradients of loss.](https://docs.microsoft.com/en-us/learn/modules/intro-machine-learning-pytorch/images/computational-graph.png)

In this network, `w` and `b` are **parameters**, which we need to optimize. Thus, we need to be able to compute the gradients of loss function with respect to those variables. In order to do that, we set the `requires_grad` property of those tensors.

> **Note:** You can set the value of `requires_grad` when creating a tensor, or later by using `x.requires_grad_(True)` method.

A function that we apply to tensors to construct computational graph is in fact an object of class `Function`. This object knows how to compute the function in the *forward* direction, and also how to compute its derivative during the *backward propagation* step. A reference to the backward propagation function is stored in `grad_fn` property of a tensor. You can find more information of `Function` [in the documentation](https://pytorch.org/docs/stable/autograd.html#function).

```python
print('Gradient function for z =',z.grad_fn)
print('Gradient function for loss =', loss.grad_fn)
```

- Queued

### Computing gradients

To optimize weights of parameters in the neural network, we need to compute the derivatives of our loss function with respect to parameters, namely, we need $\frac{∂loss}{∂w}$ and $\frac{∂loss}{∂b}$ under some fixed values of `x` and `y`. To compute those derivatives, we call `loss.backward()`, and then retrieve the values from `w.grad` and `b.grad`:

```python
loss.backward()
print(w.grad)
print(b.grad)
```

> **Note:** We can only obtain the `grad` properties for the leaf nodes of the computational graph, which have `requires_grad` property set to `True`. For all other nodes in our graph, gradients will not be available. In addition, we can only perform gradient calculations using `backward` once on a given graph, for performance reasons. If we need to do several `backward` calls on the same graph, we need to pass `retain_graph=True` to the `backward` call.

### Disabling gradient tracking

By default, all tensors with `requires_grad=True` are tracking their computational history and support gradient computation. However, there are some cases when we do not need to do that, for example, when we have trained the model and just want to apply it to some input data, i.e. we only want to do *forward* computations through the network. We can stop tracking computations by surrounding our computation code with `torch.no_grad()` block:

기본적으로 'required_grad=True'를 가진 모든 텐서는 계산 이력을 추적하고 구배 계산을 지원한다. 그러나 모델을 교육한 후 일부 입력 데이터에 적용하려는 경우와 같이 네트워크를 통해 *전달* 연산만 수행하려는 경우가 있습니다. 컴퓨팅 코드를 'torch.no_grade' 블록으로 둘러싸서 컴퓨팅 추적을 중지할 수 있습니다.

```python
z = torch.matmul(x, w)+b
print(z.requires_grad)
with torch.no_grad():
  z = torch.matmul(x, w)+b
print(z.requires_grad)
```

- Another way to achieve the same result is to use the `detach()` method on the tensor:

```python
z = torch.matmul(x, w)+b
z_det = z.detach()
print(z_det.requires_grad)
```

- There are reasons you might want to disable gradient tracking:
  - To mark some parameters in your neural network at **frozen parameters**. This is a very common scenario for [fine tuning a pre-trained network](https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html)
  - To **speed up computations** when you are only doing forward pass, because computations on tensors that do not track gradients would be more efficient.



### More on Computational Graphs

Conceptually, autograd keeps a record of data (tensors) and all executed operations (along with the resulting new tensors) in a directed acyclic graph (DAG) consisting of [Function](https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function) objects. In this DAG, leaves are the input tensors, roots are the output tensors. By tracing this graph from roots to leaves, you can automatically compute the gradients using the chain rule. 

개념적으로, 자동 격자는 데이터(측정기)와 모든 실행된 작업의 기록을 [Function](https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function) 객체로 구성된 지시 순환 그래프(DAG)에 보관한다. 이 DAG에서 리프는 입력 텐서이고 루트는 출력 텐서입니다. 이 그래프를 루트에서 리프까지 추적하면 체인 규칙을 사용하여 구배를 자동으로 계산할 수 있습니다.

In a forward pass, autograd does two things simultaneously:

- run the requested operation to compute a resulting tensor
- maintain the operation’s *gradient function* in the DAG.

The backward pass kicks off when `.backward()` is called on the DAG root. `autograd` then:

- computes the gradients from each `.grad_fn`,
- accumulates them in the respective tensor’s `.grad` attribute
- using the chain rule, propagates all the way to the leaf tensors.



**DAGs are dynamic in PyTorch**

An important thing to note is that the graph is recreated from scratch; after each `.backward()` call, autograd starts populating a new graph. This is exactly what allows you to use control flow statements in your model; you can change the shape, size and operations at every iteration if needed.

중요한 점은 그래프가 처음부터 다시 생성된다는 것입니다. 각 '.backward()' 호출 후 autograd가 새 그래프를 채우기 시작한다. 이 기능을 사용하면 모델에서 제어 흐름 문을 사용할 수 있습니다. 필요한 경우 반복할 때마다 모양, 크기 및 작업을 변경할 수 있습니다.



### Optional reading: Tensor gradients and Jacobian products

In many cases, we have a scalar loss function, and we need to compute the gradient with respect to some parameters. However, there are cases when the output function is an arbitrary tensor. In this case, PyTorch allows you to compute so-called **Jacobian product**, and not the actual gradient.

For a vector function $\vec{y}=f(\vec{x})$,  where $\vec{x}=\langle x_1,\dots,x_n\rangle$ and $\vec{y}=\langle y_1,\dots,y_m\rangle$, a gradient of $\vec{y}$ with respect to $\vec{x}$ is given by **Jacobian matrix**



Instead of computing the Jacobian matrix itself, PyTorch allows you to compute **Jacobian Product** $v^T⋅J$ for a given input vector $v=(v_1…v_m)$ This is achieved by calling `backward` with $v$ as an argument. The size of vv should be the same as the size of the original tensor, with respect to which we want to compute the product:

```python
inp = torch.eye(5, requires_grad=True)
out = (inp+1).pow(2)
out.backward(torch.ones_like(inp), retain_graph=True)
print("First call\n", inp.grad)

out.backward(torch.ones_like(inp), retain_graph=True)
print("\nSecond call\n", inp.grad)

inp.grad.zero_()
out.backward(torch.ones_like(inp), retain_graph=True)
print("\nCall after zeroing gradients\n", inp.grad)
```

- Queued

Notice that when we call `backward` for the second time with the same argument, the value of the gradient is different. This happens because when doing `backward` propagation, PyTorch **accumulates the gradients**, i.e. the value of computed gradients is added to the `grad` property of all leaf nodes of computational graph. If you want to compute the proper gradients, you need to zero out the `grad` property before. In real-life training an *optimizer* helps us to do this.

같은 주장으로 두 번째 `backward` 라고 부르면 gradient 값이 달라진다는 점에 유의해야 한다. 이는 PyTorch가 `backward` propagation 을 수행할 때 Gradient를 누적시키기 때문에 발생한다. 즉, 계산된 구배 값이 계산 그래프의 모든 리프 노드의 `grad` 속성에 추가됩니다. 적절한 구배를 계산하려면 먼저 grad 속성을 0으로 설정해야 한다. 실제 교육에서 *optimizer*는 이를 수행하는 데 도움이 됩니다.

> **Note:** previously we were calling `backward()` function without parameters. This is equivalent to calling `backward(torch.tensor(1.0))`, which is a useful way to compute the gradients in case of a scalar-valued function, such as loss during neural network training.

### Check your knowledge

1. The purpose for the `torch.autograd` engine is to:
   - automatically grade a model's accuracy
   - automatically optimize a model's internal layer structure
   - automatically optimize the set of data used to build a model
   - automatically compute gradients during model optimization
     - Correct!

------

## Learn about the optimization loop

```python
%matplotlib inline
```

### Optimizing the model parameters

Now that we have a model and data it's time to train, validate and test our model by optimizing its parameters on our data. Training a model is an iterative process; in each iteration (called an *epoch*) the model makes a guess about the output, calculates the error in its guess (*loss*), collects the derivatives of the error with respect to its parameters (as we saw in the module), and **optimizes** these parameters using gradient descent. For a more detailed walkthrough of this process, check out this video on [backpropagation from 3Blue1Brown](https://www.youtube.com/watch?v=tIeHLnjs5U8).

### Prerequisite code

We load the code from the previous modules on **Datasets & DataLoaders** and **Build Model**

```python
import torch
from torch import nn
from torch.utils.data import DataLoader
from torchvision import datasets
from torchvision.transforms import ToTensor, Lambda
training_data = datasets.FashionMNIST(
  root="data",
  train=True,
  download=True,
  transform=ToTensor()
)
test_data = datasets.FashionMNIST(
  root="data",
  train=False,
  download=True,
  transform=ToTensor()
)
train_dataloader = DataLoader(training_data, batch_size=64)
test_dataloader = DataLoader(test_data, batch_size=64)
class NeuralNetwork(nn.Module):
  def __init__(self):
    super(NeuralNetwork, self).__init__()
    self.flatten = nn.Flatten()
    self.linear_relu_stack = nn.Sequential(
      nn.Linear(28*28, 512),
      nn.ReLU(),
      nn.Linear(512, 512),
      nn.ReLU(),
      nn.Linear(512, 10),
      nn.ReLU()
    )  
	def forward(self, x):
    x = self.flatten(x)
    logits = self.linear_relu_stack(x)
    return logits

	model = NeuralNetwork()
```



### Setting hyperparameters

Hyperparameters are adjustable parameters that let you control the model optimization process. Different hyperparameter values can impact model training and convergence rates ([read more](https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html) about hyperparameter tuning)

We define the following hyperparameters for training:

- **Number of Epochs** - the number times to iterate over the dataset
- **Batch Size** - the number of data samples seen by the model in each epoch
- **Learning Rate** - how much to update models parameters at each batch/epoch. Smaller values yield slow learning speed, while large values may result in unpredictable behavior during training.

```python
learning_rate = 1e-3
batch_size = 64
epochs = 5
```

### Add an optimization loop

Once we set our hyperparameters, we can then train and optimize our model with an optimization loop. Each iteration of the optimization loop is called an **epoch**.

Each epoch consists of two main parts:

- **The Train Loop** - iterate over the training dataset and try to converge to optimal parameters.
- **The Validation/Test Loop** - iterate over the test dataset to check if model performance is improving.

Let's briefly familiarize ourselves with some of the concepts used in the training loop. Jump ahead to see the `full-impl-label` of the optimization loop.

### Add a loss function

When presented with some training data, our untrained network is likely not to give the correct answer. **Loss function** measures the degree of dissimilarity of obtained result to the target value, and it is the loss function that we want to minimize during training. To calculate the loss we make a prediction using the inputs of our given data sample and compare it against the true data label value.

Common loss functions include [nn.MSELoss](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss) (Mean Square Error) for regression tasks, and [nn.NLLLoss](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss) (Negative Log Likelihood) for classification. [nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss) combines `nn.LogSoftmax` and `nn.NLLLoss`.

We pass our model's output logits to `nn.CrossEntropyLoss`, which will normalize the logits and compute the prediction error.

```

```



\# Initialize the loss function

loss_fn = nn.CrossEntropyLoss()

**

### Optimization pass

Optimization is the process of adjusting model parameters to reduce model error in each training step. **Optimization algorithms** define how this process is performed (in this example we use Stochastic Gradient Descent). All optimization logic is encapsulated in the `optimizer` object. Here, we use the SGD optimizer; additionally, there are many [different optimizers](https://pytorch.org/docs/stable/optim.html) available in PyTorch such as ADAM and RMSProp, that work better for different kinds of models and data.

We initialize the optimizer by registering the model's parameters that need to be trained, and passing in the learning rate hyperparameter.

```

```



optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)



Inside the training loop, optimization happens in three steps:

- Call `optimizer.zero_grad()` to reset the gradients of model parameters. Gradients by default add up; to prevent double-counting, we explicitly zero them at each iteration.
- Back-propagate the prediction loss with a call to `loss.backwards()`. PyTorch deposits the gradients of the loss w.r.t. each parameter.
- Once we have our gradients, we call `optimizer.step()` to adjust the parameters by the gradients collected in the backward pass.

### Full implementation

We define `train_loop` that loops over our optimization code, and `test_loop` that evaluates the model's performance against our test data.

```python
def train_loop(dataloader, model, loss_fn, optimizer):
  size = len(dataloader.dataset)
  for batch, (X, y) in enumerate(dataloader):    
    # Compute prediction and loss
    pred = model(X)
    loss = loss_fn(pred, y) 
    # Backpropagation
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    if batch % 100 == 0:
      loss, current = loss.item(), batch * len(X)
      print(f"loss: {loss:>7f} [{current:>5d}/{size:>5d}]")

def test_loop(dataloader, model, loss_fn):
  size = len(dataloader.dataset)
  test_loss, correct = 0, 0
  with torch.no_grad():
    for X, y in dataloader:
      pred = model(X)
      test_loss += loss_fn(pred, y).item()
      correct += (pred.argmax(1) == y).type(torch.float).sum().item()
      
  test_loss /= size
  correct /= size
  print(f"Test Error: \n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \n")
```



We initialize the loss function and optimizer, and pass it to `train_loop` and `test_loop`. Feel free to increase the number of epochs to track the model's improving performance.

```python
loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)
epochs = 10
for t in range(epochs):
  print("Epoch {t+1}\n-------------------------------")
  train_loop(train_dataloader, model, loss_fn, optimizer)
  test_loop(test_dataloader, model, loss_fn)
print("Done!")
```



```
Epoch 1 ------------------------------- loss: 2.307260  [    0/60000] loss: 2.305284  [ 6400/60000] loss: 2.293966  [12800/60000] loss: 2.291592  [19200/60000] loss: 2.288022  [25600/60000] loss: 2.259277  [32000/60000] loss: 2.277950  [38400/60000] loss: 2.252569  [44800/60000] loss: 2.238333  [51200/60000] loss: 2.239141  [57600/60000] Test Error:  Accuracy: 27.5%, Avg loss: 0.035050  

Epoch 2 ------------------------------- loss: 2.222609  [    0/60000] loss: 2.244805  [ 6400/60000] loss: 2.209550  [12800/60000] loss: 2.227453  [19200/60000] loss: 2.217051  [25600/60000] loss: 2.162092  [32000/60000] loss: 2.206926  [38400/60000] loss: 2.151579  [44800/60000] loss: 2.117667  [51200/60000] loss: 2.143689  [57600/60000] Test Error:  Accuracy: 38.9%, Avg loss: 0.033368 

Epoch 3 ------------------------------- loss: 2.102783  [    0/60000] loss: 2.154025  [ 6400/60000] loss: 2.076486  [12800/60000] loss: 2.124048  [19200/60000] loss: 2.107713  [25600/60000] loss: 2.014179  [32000/60000] loss: 2.090220  [38400/60000] loss: 1.989485  [44800/60000] loss: 1.933911  [51200/60000] loss: 2.002917  [57600/60000] Test Error:  Accuracy: 41.2%, Avg loss: 0.030885  

Epoch 4 ------------------------------- loss: 1.926293  [    0/60000] loss: 2.019496  [ 6400/60000] loss: 1.888668  [12800/60000] loss: 1.987653  [19200/60000] loss: 1.968171  [25600/60000] loss: 1.838344  [32000/60000] loss: 1.951870  [38400/60000] loss: 1.808960  [44800/60000] loss: 1.749038  [51200/60000] loss: 1.868777  [57600/60000] Test Error:  Accuracy: 44.4%, Avg loss: 0.028537 

Epoch 5 ------------------------------- loss: 1.754023  [    0/60000] loss: 1.889865  [ 6400/60000] loss: 1.724985  [12800/60000] loss: 1.880932  [19200/60000] loss: 1.852289  [25600/60000] loss: 1.703095  [32000/60000] loss: 1.850078  [38400/60000] loss: 1.679640  [44800/60000] loss: 1.618462  [51200/60000] loss: 1.781099  [57600/60000] Test Error:  Accuracy: 46.4%, Avg loss: 0.026904 

Epoch 6 ------------------------------- loss: 1.629323  [    0/60000] loss: 1.794621  [ 6400/60000] loss: 1.609603  [12800/60000] loss: 1.806047  [19200/60000] loss: 1.771073  [25600/60000] loss: 1.610854  [32000/60000] loss: 1.782800  [38400/60000] loss: 1.593032  [44800/60000] loss: 1.530435  [51200/60000] loss: 1.721836  [57600/60000] Test Error:  Accuracy: 47.5%, Avg loss: 0.025738  

Epoch 7 ------------------------------- loss: 1.541017  [    0/60000] loss: 1.723998  [ 6400/60000] loss: 1.525540  [12800/60000] loss: 1.745950  [19200/60000] loss: 1.714844  [25600/60000] loss: 1.542636  [32000/60000] loss: 1.735072  [38400/60000] loss: 1.529822  [44800/60000] loss: 1.467118  [51200/60000] loss: 1.675812  [57600/60000] Test Error:  Accuracy: 48.3%, Avg loss: 0.024844  

Epoch 8 ------------------------------- loss: 1.474333  [    0/60000] loss: 1.669000  [ 6400/60000] loss: 1.460421  [12800/60000] loss: 1.694097  [19200/60000] loss: 1.674764  [25600/60000] loss: 1.487773  [32000/60000] loss: 1.699166  [38400/60000] loss: 1.481064  [44800/60000] loss: 1.419311  [51200/60000] loss: 1.638599  [57600/60000] Test Error:  Accuracy: 48.7%, Avg loss: 0.024137 

Epoch 9 ------------------------------- loss: 1.420322  [    0/60000] loss: 1.625176  [ 6400/60000] loss: 1.408073  [12800/60000] loss: 1.649715  [19200/60000] loss: 1.644693  [25600/60000] loss: 1.443653  [32000/60000] loss: 1.671596  [38400/60000] loss: 1.443777  [44800/60000] loss: 1.382555  [51200/60000] loss: 1.608089  [57600/60000] Test Error:  Accuracy: 49.1%, Avg loss: 0.023570  

Epoch 10 ------------------------------- loss: 1.375013  [    0/60000] loss: 1.588062  [ 6400/60000] loss: 1.364595  [12800/60000] loss: 1.612044  [19200/60000] loss: 1.621220  [25600/60000] loss: 1.407904  [32000/60000] loss: 1.649211  [38400/60000] loss: 1.415225  [44800/60000] loss: 1.353849  [51200/60000] loss: 1.582835  [57600/60000] Test Error:  Accuracy: 49.5%, Avg loss: 0.023104  Done! 
```

You may have noticed that the model is initially not very good (that's OK!). Try running the loop for more `epochs` or adjusting the `learning_rate` to a bigger number. It might also be the case that the model configuration we chose might not be the optimal one for this kind of problem (it isn't). Later courses will delve more into the model shapes that work for vision problems.



### Check your knowledge

1. What is the purpose of the Loss Function during model optimization?

- The Loss Function measures the degree of dissimilarity of an obtained result to the target value
- The loss function's gradient helps the optimizer with appropriate parameter adjustments during training
- The loss function is what is minimized during training
- All of the above are true



## Save, load, and run model predictions

```
%matplotlib inline
```

```
Matplotlib is building the font cache; this may take a moment. 
```

### Save and load the model

In this unit we will look at how to persist model state with saving, loading and running model predictions.

```python
import torch
import torch.onnx as onnx
import torchvision.models as models
```

### Saving and loading model weights

PyTorch models store the learned parameters in an internal state dictionary, called `state_dict`. These can be persisted via the `torch.save` method:

```python
model = models.vgg16(pretrained=True)
torch.save(model.state_dict(), 'data/model_weights.pth')
```

```
Downloading: "https://download.pytorch.org/models/vgg16-397923af.pth" to /home/mslearnuser/.cache/torch/hub/checkpoints/vgg16-397923af.pth 
```

- ​	528M/528M [00:02<00:00, 251MB/s]

To load model weights, you need to create an instance of the same model first, and then load the parameters using the `load_state_dict()` method.

```python
model = models.vgg16() # we do not specify pretrained=True, i.e. do not load default weights

model.load_state_dict(torch.load('data/model_weights.pth'))

model.eval()
```

```
VGG(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU(inplace=True)
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (6): ReLU(inplace=True)
    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (8): ReLU(inplace=True)
    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (11): ReLU(inplace=True)
    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (13): ReLU(inplace=True)
    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (15): ReLU(inplace=True)
    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (18): ReLU(inplace=True)
    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (20): ReLU(inplace=True)
    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (22): ReLU(inplace=True)
    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (25): ReLU(inplace=True)
    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (27): ReLU(inplace=True)
    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (29): ReLU(inplace=True)
    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))
  (classifier): Sequential(
    (0): Linear(in_features=25088, out_features=4096, bias=True)
    (1): ReLU(inplace=True)
    (2): Dropout(p=0.5, inplace=False)
    (3): Linear(in_features=4096, out_features=4096, bias=True)
    (4): ReLU(inplace=True)
    (5): Dropout(p=0.5, inplace=False)
    (6): Linear(in_features=4096, out_features=1000, bias=True)
  )
)
```

> **Note:** Be sure to call `model.eval()` method before inferencing to set the dropout and batch normalization layers to evaluation mode. Failing to do this will yield inconsistent inference results.

### Saving and loading models with shapes

When loading model weights, we needed to instantiate the model class first, because the class defines the structure of a network. We might want to save the structure of this class together with the model, in which case we can pass `model` (and not `model.state_dict()`) to the saving function:

```

```



torch.save(model, 'data/vgg_model.pth')

**<1초

We can then load the model like this:

```

```



model = torch.load('data/vgg_model.pth')

**<1초

> **Note:** This approach uses Python [pickle](https://docs.python.org/3/library/pickle.html) module when serializing the model, thus it relies on the actual class definition to be available when loading the model.

### Exporting the model to ONNX

PyTorch also has native ONNX export support. Given the dynamic nature of the PyTorch execution graph, however, the export process must traverse the execution graph to produce a persisted ONNX model. For this reason, a test variable of the appropriate size should be passed in to the export routine (in our case, we will create a dummy zero tensor of the correct size):

```python
input_image = torch.zeros((1,3,224,224))
onnx.export(model, input_image, 'data/model.onnx')
```

There are a lot of things you can do with ONNX model, including running inference on different platforms and in different programming languages. For more details, we recommend visiting [ONNX tutorial](https://github.com/onnx/tutorials).

Congratulations! You have completed the PyTorch beginner tutorial! We hope this tutorial has helped you get started with deep learning on PyTorch.

### Check your knowledge

1. What is a PyTorch model `state_dict`?
   - It is a model's internal state dictionary that stores its current accuracy and loss values.
   - It is a model's internal state dictionary that stores versions of the data used for training.
   - It is a model's internal state dictionary that stores its internal layers.
   - It is a model's internal state dictionary that stores learned parameters.
     - Correct!

## The full model building process

```python
%matplotlib inline
```

This unit runs through the API for common tasks in machine learning. Refer to the links in each section to dive deeper.

### Working with data

PyTorch has two [primitives to work with data](https://pytorch.org/docs/stable/data.html): `torch.utils.data.DataLoader` and `torch.utils.data.Dataset`. `Dataset` stores the samples and their corresponding labels, and `DataLoader` wraps an iterable around the `Dataset`.

```python
import torch
from torch import nn
from torch.utils.data import DataLoader
from torchvision import datasets
from torchvision.transforms import ToTensor, Lambda, Compose
import matplotlib.pyplot as plt
```

PyTorch offers domain-specific libraries such as [TorchText](https://pytorch.org/text/stable/index.html), [TorchVision](https://pytorch.org/vision/stable/index.html), and [TorchAudio](https://pytorch.org/audio/stable/index.html), all of which include datasets. For this tutorial, we will be using a TorchVision dataset.

The `torchvision.datasets` module contains `Dataset` objects for many real-world vision data like CIFAR, COCO ([full list here](https://pytorch.org/docs/stable/torchvision/datasets.html)). In this tutorial, we'll use the **FashionMNIST** dataset. Every TorchVision `Dataset` includes two arguments: `transform` and `target_transform` to modify the samples and labels respectively.

```python
# Download training data from open datasets.
training_data = datasets.FashionMNIST(
  root="data",
  train=True,
  download=True,
  transform=ToTensor(),
)

# Download test data from open datasets.
test_data = datasets.FashionMNIST(
  root="data",
  train=False,
  download=True,
  transform=ToTensor(),
)
```

```
Using downloaded and verified file: data/FashionMNIST/raw/train-images-idx3-ubyte.gz Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw Using downloaded and verified file: data/FashionMNIST/raw/train-labels-idx1-ubyte.gz Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw Using downloaded and verified file: data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw Using downloaded and verified file: data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw Processing... Done! ``/anaconda/envs/py37_pytorch/lib/python3.7/site-packages/torchvision/datasets/mnist.py:480: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1607370141920/work/torch/csrc/utils/tensor_numpy.cpp:141.)  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s) 
```

We pass the `Dataset` as an argument to `DataLoader`. This wraps an iterable over our dataset, and supports automatic batching, sampling, shuffling and multiprocess data loading. Here we define a batch size of 64, i.e. each element in the dataloader iterable will return a batch of 64 features and labels.

```python
batch_size = 64
# Create data loaders.
train_dataloader = DataLoader(training_data, batch_size=batch_size)
test_dataloader = DataLoader(test_data, batch_size=batch_size)
for X, y in test_dataloader:
  print("Shape of X [N, C, H, W]: ", X.shape)
  print("Shape of y: ", y.shape, y.dtype)
  break
```

```
Shape of X [N, C, H, W]:  torch.Size([64, 1, 28, 28]) Shape of y:  torch.Size([64]) torch.int64 
```

### Creating models

To define a neural network in PyTorch, we create a class that inherits from [nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). We define the layers of the network in the `__init__` function and specify how data will pass through the network in the `forward` function. To accelerate operations in the neural network, we move it to the GPU if available.

```python
# Get cpu or gpu device for training.
device = "cuda" if torch.cuda.is_available() else "cpu"
print("Using {} device".format(device))
# Define model
class NeuralNetwork(nn.Module):
  def __init__(self):
    super(NeuralNetwork, self).__init__()
    self.flatten = nn.Flatten()
    self.linear_relu_stack = nn.Sequential(
      nn.Linear(28*28, 512),
      nn.ReLU(),
      nn.Linear(512, 512),
      nn.ReLU(),
      nn.Linear(512, 10),
      nn.ReLU()
    )
    
 def forward(self, x):
    x = self.flatten(x)
    logits = self.linear_relu_stack(x)
    return logits
  
model = NeuralNetwork().to(device)
print(model)
```



```markdown
Using cuda device NeuralNetwork( 
	(flatten): Flatten(start_dim=1, end_dim=-1)  (linear_relu_stack): 
	Sequential(
    (0): Linear(in_features=784, out_features=512, bias=True)   
    (1): ReLU()   
    (2): Linear(in_features=512, out_features=512, bias=True)   
    (3): ReLU()    
    (4): Linear(in_features=512, out_features=10, bias=True)  
    (5): ReLU()  	
	)
) 
```

### Optimizing the Model Parameters

To train a model, we need a [loss function](https://pytorch.org/docs/stable/nn.html#loss-functions>) and an [optimizer](https://pytorch.org/docs/stable/optim.html).

```python
loss_fn = nn.CrossEntropyLoss()
learning_rate = 1e-3
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)
```

In a single training loop, the model makes predictions on the training dataset (fed to it in batches), and back-propagates the prediction error to adjust the model's parameters.

```python
def train(dataloader, model, loss_fn, optimizer):
  size = len(dataloader.dataset)
  for batch, (X, y) in enumerate(dataloader):
    X, y = X.to(device), y.to(device)
    # Compute prediction error
    pred = model(X)
    loss = loss_fn(pred, y)
    
    # Backpropagation
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    if batch % 100 == 0:
      loss, current = loss.item(), batch * len(X)
      print(f"loss: {loss:>7f} [{current:>5d}/{size:>5d}]")
```

We can also check the model's performance against the test dataset to ensure it is learning.

```python
def test(dataloader, model):
  size = len(dataloader.dataset)
  model.eval()
  test_loss, correct = 0, 0
  
  with torch.no_grad():
    for X, y in dataloader:
      X, y = X.to(device), y.to(device)
      pred = model(X)
      test_loss += loss_fn(pred, y).item()
      correct += (pred.argmax(1) == y).type(torch.float).sum().item()
      test_loss /= size

  correct /= size
  print("Test Error: \n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \n")
```

The training process is conducted over several iterations (*epochs*). During each epoch, the model learns parameters to make better predictions. We print the model's accuracy and loss at each epoch; we'd like to see the accuracy increase and the loss decrease with every epoch.

```python
epochs = 15
for t in range(epochs):
  print(f"Epoch {t+1}\n-------------------------------")
  train(train_dataloader, model, loss_fn, optimizer)
  test(test_dataloader, model)
print("Done!")
```

```
Epoch 1 ------------------------------- loss: 2.308244  [    0/60000] loss: 2.301662  [ 6400/60000] loss: 2.299381  [12800/60000] loss: 2.293565  [19200/60000] loss: 2.278485  [25600/60000] loss: 2.281788  [32000/60000] loss: 2.278154  [38400/60000] loss: 2.264884  [44800/60000] loss: 2.278050  [51200/60000] loss: 2.265731  [57600/60000] Test Error:  Accuracy: 33.0%, Avg loss: 0.035361  
Epoch 2 ------------------------------- loss: 2.263529  [    0/60000] loss: 2.243206  [ 6400/60000] loss: 2.234885  [12800/60000] loss: 2.242897  [19200/60000] loss: 2.183212  [25600/60000] loss: 2.204664  [32000/60000] loss: 2.196060  [38400/60000] loss: 2.165953  [44800/60000] loss: 2.213932  [51200/60000] loss: 2.189297  [57600/60000] Test Error:  Accuracy: 44.4%, Avg loss: 0.033856  
Epoch 3 ------------------------------- loss: 2.191511  [    0/60000] loss: 2.151078  [ 6400/60000] loss: 2.128932  [12800/60000] loss: 2.153729  [19200/60000] loss: 2.017538  [25600/60000] loss: 2.086752  [32000/60000] loss: 2.070010  [38400/60000] loss: 2.020661  [44800/60000] loss: 2.123916  [51200/60000] loss: 2.086038  [57600/60000] Test Error:  Accuracy: 45.5%, Avg loss: 0.031705  
Epoch 4 ------------------------------- loss: 2.092045  [    0/60000] loss: 2.025146  [ 6400/60000] loss: 1.990673  [12800/60000] loss: 2.033067  [19200/60000] loss: 1.811971  [25600/60000] loss: 1.952929  [32000/60000] loss: 1.915350  [38400/60000] loss: 1.853108  [44800/60000] loss: 2.003140  [51200/60000] loss: 1.930078  [57600/60000] Test Error:  Accuracy: 46.9%, Avg loss: 0.029158  
Epoch 5 ------------------------------- loss: 1.977102  [    0/60000] loss: 1.892723  [ 6400/60000] loss: 1.856888  [12800/60000] loss: 1.868795  [19200/60000] loss: 1.605191  [25600/60000] loss: 1.815202  [32000/60000] loss: 1.708351  [38400/60000] loss: 1.687288  [44800/60000] loss: 1.848937  [51200/60000] loss: 1.731334  [57600/60000] Test Error:  Accuracy: 52.0%, Avg loss: 0.026580  
Epoch 6 ------------------------------- loss: 1.853199  [    0/60000] loss: 1.764611  [ 6400/60000] loss: 1.737372  [12800/60000] loss: 1.711567  [19200/60000] loss: 1.431005  [25600/60000] loss: 1.696275  [32000/60000] loss: 1.523592  [38400/60000] loss: 1.551817  [44800/60000] loss: 1.700327  [51200/60000] loss: 1.556069  [57600/60000] Test Error:  Accuracy: 54.7%, Avg loss: 0.024369 
Epoch 7 ------------------------------- loss: 1.732549  [    0/60000] loss: 1.649983  [ 6400/60000] loss: 1.629636  [12800/60000] loss: 1.581321  [19200/60000] loss: 1.292695  [25600/60000] loss: 1.598631  [32000/60000] loss: 1.379164  [38400/60000] loss: 1.447635  [44800/60000] loss: 1.583671  [51200/60000] loss: 1.433749  [57600/60000] Test Error:  Accuracy: 55.2%, Avg loss: 0.022687 
Epoch 8 ------------------------------- loss: 1.632018  [    0/60000] loss: 1.558001  [ 6400/60000] loss: 1.538675  [12800/60000] loss: 1.487822  [19200/60000] loss: 1.193841  [25600/60000] loss: 1.522596  [32000/60000] loss: 1.279208  [38400/60000] loss: 1.372921  [44800/60000] loss: 1.502596  [51200/60000] loss: 1.354262  [57600/60000] Test Error:  Accuracy: 55.8%, Avg loss: 0.021514 
Epoch 9 ------------------------------- loss: 1.555871  [    0/60000] loss: 1.491291  [ 6400/60000] loss: 1.469144  [12800/60000] loss: 1.425747  [19200/60000] loss: 1.128848  [25600/60000] loss: 1.467914  [32000/60000] loss: 1.212620  [38400/60000] loss: 1.320562  [44800/60000] loss: 1.446733  [51200/60000] loss: 1.302307  [57600/60000] Test Error:  Accuracy: 56.7%, Avg loss: 0.020715  
Epoch 10 ------------------------------- loss: 1.500138  [    0/60000] loss: 1.444472  [ 6400/60000] loss: 1.417692  [12800/60000] loss: 1.384599  [19200/60000] loss: 1.086298  [25600/60000] loss: 1.428751  [32000/60000] loss: 1.166260  [38400/60000] loss: 1.283096  [44800/60000] loss: 1.406363  [51200/60000] loss: 1.266232  [57600/60000] Test Error:  Accuracy: 57.6%, Avg loss: 0.020150  
Epoch 11 ------------------------------- loss: 1.458530  [    0/60000] loss: 1.410509  [ 6400/60000] loss: 1.379321  [12800/60000] loss: 1.355739  [19200/60000] loss: 1.056411  [25600/60000] loss: 1.399136  [32000/60000] loss: 1.132577  [38400/60000] loss: 1.255147  [44800/60000] loss: 1.375376  [51200/60000] loss: 1.240256  [57600/60000] Test Error:  Accuracy: 58.4%, Avg loss: 0.019722 
Epoch 12 ------------------------------- loss: 1.425603  [    0/60000] loss: 1.383455  [ 6400/60000] loss: 1.349400  [12800/60000] loss: 1.334092  [19200/60000] loss: 1.034445  [25600/60000] loss: 1.376258  [32000/60000] loss: 1.106107  [38400/60000] loss: 1.232482  [44800/60000] loss: 1.349747  [51200/60000] loss: 1.219756  [57600/60000] Test Error:  Accuracy: 59.2%, Avg loss: 0.019377  
Epoch 13 ------------------------------- loss: 1.397835  [    0/60000] loss: 1.360644  [ 6400/60000] loss: 1.324387  [12800/60000] loss: 1.316506  [19200/60000] loss: 1.017424  [25600/60000] loss: 1.357446  [32000/60000] loss: 1.081942  [38400/60000] loss: 1.213508  [44800/60000] loss: 1.327295  [51200/60000] loss: 1.202648  [57600/60000] Test Error:  Accuracy: 60.1%, Avg loss: 0.019083 
Epoch 14 ------------------------------- loss: 1.373411  [    0/60000] loss: 1.340207  [ 6400/60000] loss: 1.303139  [12800/60000] loss: 1.301876  [19200/60000] loss: 1.002764  [25600/60000] loss: 1.342115  [32000/60000] loss: 1.060629  [38400/60000] loss: 1.196334  [44800/60000] loss: 1.306820  [51200/60000] loss: 1.187618  [57600/60000] Test Error:  Accuracy: 60.8%, Avg loss: 0.018823  
Epoch 15 ------------------------------- loss: 1.351294  [    0/60000] loss: 1.321608  [ 6400/60000] loss: 1.284594  [12800/60000] loss: 1.289170  [19200/60000] loss: 0.990012  [25600/60000] loss: 1.327834  [32000/60000] loss: 1.041283  [38400/60000] loss: 1.181126  [44800/60000] loss: 1.287974  [51200/60000] loss: 1.174571  [57600/60000] Test Error:  Accuracy: 61.2%, Avg loss: 0.018589  Done! 
```

The accuracy will initially not be very good (that's OK!). Try running the loop for more `epochs` or adjusting the `learning_rate` to a bigger number. It might also be the case that the model configuration we chose might not be the optimal one for this kind of problem (it isn't). Later courses will delve more into the model shapes that work for vision problems.

### Saving Models

A common way to save a model is to serialize the internal state dictionary (containing the model parameters).

```python
torch.save(model.state_dict(), "data/model.pth")
print("Saved PyTorch Model State to model.pth")
```



```
Saved PyTorch Model State to model.pth 
```

### Loading Models

The process for loading a model includes re-creating the model structure and loading the state dictionary into it.

```python
model = NeuralNetwork()
model.load_state_dict(torch.load("data/model.pth"))
```



```
<All keys matched successfully>
```

This model can now be used to make predictions.

```python
classes = [
  "T-shirt/top",
  "Trouser",
  "Pullover",
  "Dress",
  "Coat",
  "Sandal",
  "Shirt",
  "Sneaker",
  "Bag",
  "Ankle boot",
]

model.eval()
x, y = test_data[0][0], test_data[0][1]
with torch.no_grad():
  pred = model(x)
  predicted, actual = classes[pred[0].argmax(0)], classes[y]
  print(f'Predicted: "{predicted}", Actual: "{actual}"')
```

```
Predicted: "Ankle boot", Actual: "Ankle boot" 
```

---

## Summary

- 1 minute

In this module we introduced the key concepts to building machine learning models and implemented those concepts with PyTorch. We built a Computer Vision model that could classify images of T-shirt/top, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, and Ankle boots. Now that you learned some of the basics of machine learning concepts, keep working through the Microsoft Learn content to learn more about building different types of machine learning models with PyTorch.


